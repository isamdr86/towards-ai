{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isamdr86/towards-ai/blob/main/notebooks/12-Improve_Query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QPJzr-I9XQ7l",
        "outputId": "f329dcce-ce75-4822-8eb7-837154cdd633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.7/410.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.3/261.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.37.0 llama-index-finetuning llama-index-embeddings-huggingface llama-index-embeddings-cohere llama-index-readers-web cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text sentence_transformers pydantic llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1 llama-index-llms-gemini==0.1.11"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 tiktoken==0.7.0 --force-reinstall --quiet"
      ],
      "metadata": {
        "id": "eOgBWTtuDFMZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "1f3f20f8cd2f455fba29f8ed87aafa2a",
            "952fe494dc9b4779afe29ca92db7cc83",
            "eaa196f2fb29452c8014dab40fdce3f6",
            "5da3d5c9fec148e693c2dad5cb380cfa",
            "7f3774ff6216492c959bfb06cf3ea154",
            "1c4e286776574a2c81b09c4c4fd966f1",
            "d9aca64c34c54ab189d694f64150ed40",
            "9d98a2764f3d4db08a7225d40dcdcd85",
            "855afc704cb7413cb6fd3de5de997b0b",
            "91eeba72a72841fb9b7b5392cabb46db",
            "b7d4d978b2834e02b81318356a0bea2f"
          ]
        },
        "id": "5sP-_zZso337",
        "outputId": "4be10a1d-4cd7-40ae-de5a-a5dc1af5071d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f3f20f8cd2f455fba29f8ed87aafa2a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodY2Xpf_kxg",
        "outputId": "5322544b-59d1-4297-c2bb-15539be3a38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create the vector index\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLrn8A3jckmW"
      },
      "source": [
        "# Multi-Step Query Engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmpfpVCje8h3"
      },
      "source": [
        "## GPT-4o-mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8y-Ya3GyfcAk"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform, #generate subqueries from original query\n",
        ")\n",
        "\n",
        "step_decompose_transform_gpt4o = StepDecomposeQueryTransform(verbose=True, llm=Settings.llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zntXdSbGf_qF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
        "\n",
        "#Default query engine\n",
        "query_engine_gpt4o_mini = vector_index.as_query_engine()\n",
        "\n",
        "# Multi Step Query Engine\n",
        "multi_step_query_engine = MultiStepQueryEngine(\n",
        "    query_engine = query_engine_gpt4o_mini,\n",
        "    query_transform = step_decompose_transform_gpt4o,\n",
        "    index_summary = \"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JPD8yAinVSq"
      },
      "source": [
        "# Query Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2IByQ5-ox9U"
      },
      "source": [
        "## Default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b0gue7cyctt1",
        "outputId": "0ed47273-ff64-4d6a-8bc2-7f5a37438041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The provided information does not include details about the Llama 3.1 Model or BERT. However, it does discuss the LLaMA model and the PEFT (Parameter-Efficient Fine-Tuning) library, which are relevant to fine-tuning and adapting models for various tasks.\n",
            "\n",
            "The LLaMA model is a foundational model used for natural language processing tasks, and it can be fine-tuned using methods like LoRA (Low-Rank Adaptation) through the PEFT library. This library offers tools for efficient fine-tuning, allowing users to adapt the LLaMA model with minimal additional parameters and reduced training time.\n",
            "\n",
            "Additionally, the Llama-Adapter is a specific PEFT method designed to transform the LLaMA model into an instruction-following model by integrating learnable adaptation prompts while preserving the model's pre-trained knowledge. This method is efficient, requiring only a small number of learnable parameters and a short fine-tuning duration.\n",
            "\n",
            "For more detailed information about Llama 3.1 and BERT, additional context would be necessary.\n"
          ]
        }
      ],
      "source": [
        "# Default query engine\n",
        "query_engine = vector_index.as_query_engine()\n",
        "res = query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465dH4yQc7Ct",
        "outputId": "90524b50-3181-4d86-b981-d1956a117982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 781b7b12-eca2-47c0-a66e-9d6be670e951\n",
            "Title\t LLaMA\n",
            "Text\t on how to fine-tune LLaMA model using LoRA method via the 🤗 PEFT library with intuitive UI. 🌎 - A [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb) on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. 🌎 ## LlamaConfig[[autodoc]] LlamaConfig## LlamaTokenizer[[autodoc]] LlamaTokenizer    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - save_vocabulary## LlamaTokenizerFast[[autodoc]] LlamaTokenizerFast    - build_inputs_with_special_tokens    - get_special_tokens_mask    - create_token_type_ids_from_sequences    - update_post_processor    - save_vocabulary## LlamaModel[[autodoc]] LlamaModel    - forward## LlamaForCausalLM[[autodoc]] LlamaForCausalLM    - forward## LlamaForSequenceClassification[[autodoc]] LlamaForSequenceClassification    - forward## LlamaForQuestionAnswering[[autodoc]] LlamaForQuestionAnswering    - forward## LlamaForTokenClassification[[autodoc]] LlamaForTokenClassification    - forward## FlaxLlamaModel[[autodoc]] FlaxLlamaModel    - __call__## FlaxLlamaForCausalLM[[autodoc]] FlaxLlamaForCausalLM    - __call__\n",
            "Score\t 0.4276423038056521\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e1e3e842-7160-40c4-8e74-772fb8254f5e\n",
            "Title\t Llama-Adapter\n",
            "Text\t # Llama-Adapter[Llama-Adapter](https://hf.co/papers/2303.16199) is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model.The abstract from the paper is:*We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapter*.## AdaptionPromptConfig[[autodoc]] tuners.adaption_prompt.config.AdaptionPromptConfig## AdaptionPromptModel[[autodoc]] tuners.adaption_prompt.model.AdaptionPromptModel\n",
            "Score\t 0.42746646264090477\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y2AiInmpz7g"
      },
      "source": [
        "## GPT-4o-mini Multi-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69kADAFilW1n",
        "outputId": "773c93df-950a-4b7b-8a18-6be0c924dca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What is the Llama 3.1 Model?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key features of the Llama 3.1 Model?\n",
            "\u001b[0mLlama 3.1 is an advanced open-source AI model developed by Meta, recognized for its significant scale and capabilities. It is the largest in the Llama series, having been trained on over 15 trillion tokens with the help of more than 16,000 H100 GPUs. One of its standout features is a context length of 128K, which allows it to process and understand longer texts effectively. The model excels in reasoning, coding, and multilingual processing, with approximately 50% of its training data consisting of multilingual tokens. It demonstrates strong logical reasoning and problem-solving skills, making it proficient in generating high-quality code. Additionally, Llama 3.1 supports zero-shot tool use and has shown superior performance in benchmark tests compared to other models, particularly in areas like mathematical reasoning and complex reasoning.\n",
            "\n",
            "BERT, or Bidirectional Encoder Representations from Transformers, is another influential model in the field of natural language processing. Developed by Google, BERT introduced a novel approach by using a bidirectional training method, allowing the model to consider the context from both directions (left and right) when processing text. This capability significantly improved its understanding of the nuances of language, making it particularly effective for tasks such as question answering and sentiment analysis. BERT has been widely adopted and serves as a foundation for many subsequent models and applications in NLP.\n",
            "\n",
            "PEFT, or Parameter-Efficient Fine-Tuning, refers to techniques designed to fine-tune large pre-trained models like Llama 3.1 and BERT with fewer parameters. This approach allows for adapting these models to specific tasks or domains without the need for extensive computational resources. PEFT methods can include techniques like adapters, which insert small trainable modules into the model, or prompt tuning, which optimizes the input prompts to elicit better responses from the model. By leveraging PEFT, practitioners can achieve high performance on specialized tasks while maintaining the efficiency and scalability of large language models. \n",
            "\n",
            "Together, Llama 3.1, BERT, and PEFT represent significant advancements in the field of AI and natural language processing, each contributing unique strengths and capabilities to the landscape.\n"
          ]
        }
      ],
      "source": [
        "response = multi_step_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGmHP72Iu10s",
        "outputId": "f05183af-cfa7-4896-a4fa-0b7db48c307a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**What is the Llama 3.1 Model?**\n",
            "Llama 3.1 is an advanced open-source AI model developed by Meta, recognized as the largest in the Llama series, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. It features a 128K context length and enhanced capabilities in reasoning, coding, and multilingual processing. The model supports zero-shot tool use and is designed to generate high-quality code while demonstrating strong logical reasoning and problem-solving skills. Llama 3.1 has shown superior performance in benchmark tests compared to other models like GPT-4o and Claude 3.5 Sonnet, particularly in areas such as mathematical reasoning, complex reasoning, and long text processing.\n",
            "\n",
            "**What are the key features of the Llama 3.1 Model?**\n",
            "The Llama 3.1 model boasts several key features, including:\n",
            "\n",
            "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
            "\n",
            "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to process and understand longer texts.\n",
            "\n",
            "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
            "\n",
            "4. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, allowing it to effectively understand and process multiple languages.\n",
            "\n",
            "5. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical skills.\n",
            "\n",
            "6. **Support for Tool Use**: The model can engage in zero-shot tool use and develop agentic behaviors.\n",
            "\n",
            "7. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 Sonnet in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
            "\n",
            "These features collectively position Llama 3.1 as a powerful and versatile language model in the AI landscape.\n",
            "\n",
            "**What are the key features of the Llama 3.1 Model?**\n",
            "The Llama 3.1 model boasts several key features, including:\n",
            "\n",
            "1. **Model Scale and Training**: It is the largest model from Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs.\n",
            "\n",
            "2. **Extended Context Length**: The model supports a context length of 128K, enhancing its ability to handle longer inputs.\n",
            "\n",
            "3. **Enhanced Capabilities**: It demonstrates improved reasoning and coding abilities compared to its predecessors.\n",
            "\n",
            "4. **Multilingual Processing**: Approximately 50% of its training data consists of multilingual tokens, allowing it to effectively understand and process multiple languages.\n",
            "\n",
            "5. **Programming and Reasoning Skills**: Llama 3.1 excels in generating high-quality code and exhibits strong logical reasoning, problem-solving, and analytical skills.\n",
            "\n",
            "6. **Support for Tool Use**: The model can engage in zero-shot tool use and develop agentic behaviors.\n",
            "\n",
            "7. **Benchmark Performance**: It outperforms competing models like GPT-4o and Claude 3.5 Sonnet in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
            "\n",
            "These features collectively position Llama 3.1 as a powerful and versatile language model in the AI landscape.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query, response in response.metadata['sub_qa']:\n",
        "    print(f\"**{query}**\\n{response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5pJPBPRqjbG",
        "outputId": "3ba3f07a-d471-47ab-e2b7-0ea17e7160ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t f5453fa3-1b20-4c2e-8549-9882cc954df3\n",
            "Text\t Llama 3.1 models  especially the 405 billion parameter version (also the 70B  8B)?   For the 405B parameter version  substantial GPU resources are required  up to 16K H100 GPUs for training  with 80GB HBM3 memory each  connected via NVLink within servers equipped with eight GPUs and two CPUs. Smaller versions (70B  8B) have lower resource requirements  using Nvidia Quantum2 InfiniBand fabric with 400 Gbps interconnects between GPUs  making them more accessible for many organizations  while storage requirements include a distributed file system offering up to 240 PB of storage with a peak throughput of 7 TB/s. Recently  Elie Bakouch (known for training LLMs on Hugging Face) shared that one can fine-tune Llama 3 405B using 8 H100 GPUs.   5  What specific advantages does Llama 3.1 offer in terms of performance  cost  and potential cost savings compared to closed models like GPT-4o?   Llama 3.1 offers significant advantages in performance  matching or surpassing GPT-4 in many benchmarks  while being more economical to run  with inference operations costing roughly 50% less than comparable closed models like GPT-4o according to an interview with Mark Zuckerberg. The open-source nature allows for more efficient customization and fine-tuning  potentially leading to better performance on specific tasks at a lower cost compared to closed models  while the ability to run the model on-premises or on preferred cloud providers gives organizations more control over their infrastructure costs.   6  What kind of skills/team does it take to work with Llama models effectively for our specific use cases?   a  For Fine-tuning  training  distilling   A team needs expertise in machine learning  particularly in natural language processing and transformer architectures. Skills in data preprocessing  model optimization  and distributed computing are crucial. Knowledge of PyTorch and experience with large-scale model training is essential. The team should include ML engineers  ML ops specialists  and developers.   b  For Deploying/using out-of-the-box   For deploying and using Llama models out-of-the-box  the necessary skills shift towards software development and cloud services expertise.\n",
            "Score\t 0.4431615518589859\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t fc78a10c-1358-435e-9a68-c21718fac23b\n",
            "Text\t not yet been widely released.   Benchmark ResultsMeta compared the Llama 3.1 405B model with models such as GPT-4  GPT-4o  and Claude 3.5 sonnet. The results showed that Llama 3.1 performed better than GPT-4o and Claude 3.5 sonnet on test data sets such as mathematical reasoning  complex reasoning  and Multilingual support and its long text processing ability is also excellent  receiving 95.2 points in zero scrolls quality.   it falls short compared to Claude 3.5 sonnet in tool utilization ability (BFCL  Nexus).   Although the performance on test data sets such as Multi-task Language Understanding  Human Eval  and MATH is slightly inferior to the closed-source model  the score difference is not large.   In addition  manual evaluation results show that the output performance of the Llama 3.1 405B model is comparable to GPT-4 and Claude 3.5 Sonnet  and slightly inferior to GPT-4o.   Just looking at this benchmark score  it seems to be quite promising.The benchmark results show that llama 3.1 405B is an excellent language model with strong language modeling capabilities  mathematical reasoning capabilities  complex reasoning and long text processing capabilities. however  there is still room for improvement in tool utilization capabilities and multilingual support.   Now that Ive introduced the benchmark scores  Im going to try using them and see how they perform.   How to Use Llama 3.1 40 5B Locally?Ollama is the fastest way to get up and running With local language models We recommend trying Llama 3.1 8b  which is impressive for its size and will perform well on most hardware.   Download Ollama here (it should walk you through the rest of these steps)Open a terminal and run ollama run llama3.1-8bGroq is now hosting the Llama 3.1 models  including the 70B and 8B models. Earlier  they offered the largest 405B model  but it has been temporarily removed due to high traffic and server\n",
            "Score\t 0.42551143457577645\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwcSCiMhp4Uh"
      },
      "source": [
        "# Test gemini-1.5-flash Multi-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "uH9gNfZuslHK",
        "outputId": "f32fdc56-8134-48e1-91eb-501fd188f404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-27263bfa0de9>:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gemini = ServiceContext.from_defaults(llm=llm)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import ServiceContext\n",
        "from llama_index.core.indices.query.query_transform.base import (\n",
        "    StepDecomposeQueryTransform,\n",
        ")\n",
        "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini(model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "service_context_gemini = ServiceContext.from_defaults(llm=llm)\n",
        "\n",
        "step_decompose_transform = StepDecomposeQueryTransform(llm=llm, verbose=True)\n",
        "\n",
        "query_engine_gemini = vector_index.as_query_engine(\n",
        "    service_context=service_context_gemini\n",
        ")\n",
        "query_engine_gemini = MultiStepQueryEngine(\n",
        "    query_engine=query_engine_gemini,\n",
        "    query_transform=step_decompose_transform,\n",
        "    index_summary=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "9s6SkHI0p6VZ",
        "outputId": "fab80e05-5d52-4b5e-c188-a28d22cdf6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are Llama 3.1, BERT, and PEFT, and how do they relate to RAG, Machine Learning, Deep Learning, and Generative AI?\n",
            "\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: What are the key characteristics and applications of Llama 3.1, BERT, and PEFT within the context of machine learning, deep learning, and generative AI?\n",
            "\n",
            "\u001b[0m\u001b[1;3;33m> Current query: Write about Llama 3.1 Model, BERT and PEFT\n",
            "\u001b[0m\u001b[1;3;38;5;200m> New query: None\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response_gemini = query_engine_gemini.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "FlgMkAhQsTIY",
        "outputId": "e2c54fbc-9b10-4e2e-d322-43d2a0d8bdb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 is an open-source language model that has been optimized for performance and cost efficiency. It incorporates techniques such as weight pruning and knowledge distillation, resulting in a more compact version known as Llama-3.1-Minitron. This model is designed to maintain high performance while minimizing computational requirements, making it suitable for a range of applications in natural language processing. Its architecture supports various parameter sizes, with larger versions offering significant capabilities for high-performance tasks.\\n\\nBERT, or Bidirectional Encoder Representations from Transformers, is a prominent language model that utilizes a transformer architecture to understand the context of words within sentences. This capability allows BERT to excel in various natural language processing tasks, including sentiment analysis, question answering, and language translation. Its bidirectional approach enables a deeper understanding of context, which is crucial for accurately interpreting language.\\n\\nPEFT, or Parameter-Efficient Fine-Tuning, is a method that focuses on fine-tuning large models by targeting a subset of their parameters. This approach is particularly advantageous in scenarios where computational resources are limited, as it allows for effective model adaptation without the need for extensive retraining. PEFT enhances the efficiency of model fine-tuning, making it a valuable technique in the realm of generative AI and machine learning.\\n\\nTogether, Llama 3.1, BERT, and PEFT represent significant advancements in the fields of machine learning and deep learning, contributing to improved model performance, cost reduction, and greater accessibility for customization in various applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "response_gemini.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxOF2qth1gUC"
      },
      "source": [
        "## Test Retriever on Multistep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "In9BZbU10KAz"
      },
      "outputs": [],
      "source": [
        "# import llama_index\n",
        "# from llama_index.core.indices.query.schema import QueryBundle\n",
        "\n",
        "# t = QueryBundle(\"How Retrieval Augmented Generation (RAG) work?\")\n",
        "# query_engine_gemini.retrieve(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2xI2YPowYpd"
      },
      "source": [
        "## Subquestion Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4JSpCzFwWG2",
        "outputId": "9431cec3-df54-4d8a-d8f9-6cde21b2f2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 5 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: What are the key features and improvements of the Llama 3.1 model compared to its predecessors?\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] Q: How does BERT work and what are its main applications in natural language processing?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] Q: What is PEFT (Parameter-Efficient Fine-Tuning) and how does it enhance the performance of models like BERT?\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] Q: What are the differences in architecture between Llama 3.1 and BERT?\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] Q: In what scenarios is PEFT particularly beneficial for fine-tuning models?\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m[LlamaIndex] A: The provided information does not detail the architectural differences between Llama 3.1 and BERT. It primarily focuses on the specifications, performance, and advantages of Llama 3.1, particularly its 405 billion parameter version, as well as its open-source nature and hardware requirements. For a comprehensive comparison of architectures, additional resources would be needed.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: PEFT is particularly beneficial for fine-tuning models in scenarios where computational resources are limited, such as when using consumer hardware. It allows for the adaptation of large pretrained models without the need to fine-tune all parameters, significantly reducing both computational and storage costs. This makes it accessible for users who may not have access to powerful GPUs or TPUs, enabling them to train and store large language models more efficiently. Additionally, PEFT methods can be advantageous in situations where quick training and deployment are required, as they streamline the process by focusing on a smaller number of trainable parameters.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[LlamaIndex] A: BERT, which stands for Bidirectional Encoder Representations from Transformers, operates using a transformer architecture that processes text bidirectionally. This means it can consider the entire context of a sentence, allowing it to capture nuances and meanings more effectively than traditional unidirectional models that read text sequentially. \n",
            "\n",
            "The model is pre-trained on two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, BERT predicts masked tokens in a sentence, learning to generate contextualized word representations based on surrounding words. NSP involves predicting whether one sentence is likely to follow another, enhancing the model's understanding of sentence relationships.\n",
            "\n",
            "BERT's architecture and training enable it to excel in various natural language processing applications, including sentiment analysis, question answering, and language translation. Its ability to understand context makes it particularly effective for tasks that require a deep comprehension of language.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[LlamaIndex] A: The Llama 3.1 model introduces several key features and improvements over its predecessors, Llama 1 and Llama 2. Notable advancements include:\n",
            "\n",
            "1. **Increased Scale and Training**: It is the largest model developed by Meta, trained on over 15 trillion tokens using more than 16,000 H100 GPUs, marking a significant upgrade in training scale.\n",
            "\n",
            "2. **Extended Context Length**: Llama 3.1 features a 128K context length, allowing for better handling of longer texts and more complex interactions.\n",
            "\n",
            "3. **Enhanced Reasoning and Coding Capabilities**: The model demonstrates improved logical reasoning, problem-solving, and coding skills, enabling it to generate high-quality code and manage intricate programming tasks effectively.\n",
            "\n",
            "4. **Multilingual Processing**: With about 50% of its training data consisting of multilingual tokens, Llama 3.1 excels in understanding and processing multiple languages.\n",
            "\n",
            "5. **Tool Use and Agentic Behaviors**: The model supports zero-shot tool use and can develop agentic behaviors, enhancing its functionality in various applications.\n",
            "\n",
            "6. **Benchmark Performance**: Llama 3.1 outperforms previous models and competitors like GPT-4o and Claude 3.5 sonnet in areas such as mathematical reasoning, complex reasoning, multilingual support, and long text processing.\n",
            "\n",
            "These improvements position Llama 3.1 as a leading open-source AI model with advanced capabilities compared to its predecessors.\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[LlamaIndex] A: PEFT (Parameter-Efficient Fine-Tuning) is a library designed to adapt large pretrained models to various downstream applications efficiently. It achieves this by fine-tuning only a small number of additional model parameters, which significantly reduces both computational and storage costs while maintaining performance levels comparable to fully fine-tuned models. This efficiency makes it more accessible for training and storing large language models on consumer hardware.\n",
            "\n",
            "While the specific mention of BERT is not included, the principles of PEFT can be applied to models like BERT, enhancing their performance by allowing for targeted fine-tuning without the need to adjust all model parameters. This approach not only streamlines the training process but also facilitates the integration of PEFT with popular libraries, thereby improving the overall training and inference experience for large models.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "query_engine = vector_index.as_query_engine()\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"LlamaIndex\",\n",
        "            description=\"Used to answer the Questions about RAG, Machine Learning, Deep Learning, and Generative AI\",\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,\n",
        "    use_async=True,\n",
        ")\n",
        "\n",
        "response = sub_question_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "IKxgCfJawjcj",
        "outputId": "89d53609-da33-42c4-aef7-042ca749c1b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 is a state-of-the-art AI model developed by Meta, notable for its significant advancements over previous versions. It boasts an impressive scale, having been trained on over 15 trillion tokens with more than 16,000 H100 GPUs. This model features a 128K context length, enhancing its ability to process longer texts and complex interactions. Additionally, Llama 3.1 demonstrates improved reasoning and coding capabilities, excels in multilingual processing, and supports zero-shot tool use, making it versatile for various applications. Its performance benchmarks indicate superiority over earlier models and competitors in areas such as mathematical reasoning and long text processing.\\n\\nBERT, or Bidirectional Encoder Representations from Transformers, utilizes a transformer architecture that processes text in both directions, allowing for a comprehensive understanding of context. It is pre-trained on tasks like Masked Language Modeling and Next Sentence Prediction, which help it generate contextualized word representations and understand sentence relationships. BERT is widely applied in natural language processing tasks, including sentiment analysis, question answering, and language translation, due to its ability to grasp nuanced meanings.\\n\\nParameter-Efficient Fine-Tuning (PEFT) is a technique designed to adapt large pretrained models like BERT for specific applications efficiently. By fine-tuning only a small subset of parameters, PEFT reduces computational and storage costs while maintaining performance levels similar to fully fine-tuned models. This approach is particularly beneficial in scenarios with limited computational resources, making it accessible for users with consumer hardware. PEFT streamlines the training process, allowing for quicker adaptation and deployment of large language models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdPwVAQ6ixg"
      },
      "source": [
        "# HyDE Transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1x6He0T961Kg"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0GgtfeBC6m0H"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine.transform_query_engine import TransformQueryEngine\n",
        "\n",
        "hyde = HyDEQueryTransform(include_original=True) # The include_original argument decides whether to include the original query string as one of the embedding strings during retrieval.\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mm3nYnIE6mwl"
      },
      "outputs": [],
      "source": [
        "response = hyde_query_engine.query(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "PjTJ2poc6mt5",
        "outputId": "cf68d61a-95c2-43cc-defc-39af51f45270"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 3.1 405B is a significant advancement in the field of AI, developed by Meta. It stands out as the largest open-source model to date, trained on over 15 trillion tokens using more than 16,000 H100 GPUs. This extensive training has enabled it to achieve a 128K context length, which enhances its capabilities in reasoning, coding, and multilingual processing. The model has been designed to perform on par with leading proprietary models in various areas, including general knowledge, steerability, and tool use.\\n\\nIn contrast, BERT (Bidirectional Encoder Representations from Transformers) is a model developed by Google that focuses on understanding the context of words in a sentence by looking at the words that come before and after it. BERT has been widely used for tasks such as natural language understanding and has set benchmarks in various NLP tasks.\\n\\nPEFT (Parameter-Efficient Fine-Tuning) refers to techniques that allow models to be fine-tuned with fewer parameters, making the process more efficient and less resource-intensive. This approach is particularly useful for adapting large models like Llama 3.1 or BERT to specific tasks without the need for extensive computational resources.\\n\\nWhile Llama 3.1 excels in multilingual processing and complex reasoning, BERT has been foundational in advancing natural language understanding. PEFT techniques can be applied to both models to enhance their performance on specific tasks while maintaining efficiency.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StgikqWZ6mrl",
        "outputId": "0218e477-c292-47c0-dd73-0b26714718ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 5624cdc8-2997-4e4d-82d1-c7383d389215\n",
            "Text\t 3.1 405B is Metas largest model  trained with over 15 trillion tokens. For this  Meta optimized the entire training stack and trained it on more than 16 000 H100 GPUs  making it the first Llama model trained at this scale.   According to Meta  this version of the original model (Llama 1 and Llama 2) has 128K context length  improved reasoning and coding capabilities. Meta has also upgraded both multilingual 8B and 70B models.   Key Features of Llama 3.1 40 5B:Llama 3.1 comes with a host of features and capabilities that appeal to The users  such as:   RAG & tool use  Meta states that you can use Llama system components to extend the model using zero-shot tool use and build agentic behaviors with RAG.   Multi-lingual  Llama 3 naturally supports multilingual processing. The pre-training data includes about 50% multilingual tokens and can process and understand multiple languages.   Programming and Reasoning  Llama 3 has powerful programming capabilities  generating high-quality code with a strong understanding of syntax and logic. It can create complex code structures and perform well in various programming tasks. Llama 3 excels in logical reasoning  problem-solving  analysis  and inference. It handles complex logical tasks and solves intricate problems effectively.   Multimodal Models  Multimodal models have been developed that support image recognition  video recognition  and speech understanding capabilities  but these models are still under development and have not yet been widely released.   Benchmark ResultsMeta compared the Llama 3.1 405B model with models such as GPT-4  GPT-4o  and Claude 3.5 sonnet. The results showed that Llama 3.1 performed better than GPT-4o and Claude 3.5 sonnet on test data sets such as mathematical reasoning  complex reasoning  and Multilingual support and its long text processing ability is also excellent  receiving 95.2 points in zero scrolls quality.   it falls short compared to Claude 3.5 sonnet in tool utilization ability\n",
            "Score\t 0.5473492672471654\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 309bb3c8-d52c-4571-b42d-7a2cde27fac4\n",
            "Text\t the AI news in the past 7 days has been insane  with so much happening in the world of AI   in this video  were diving into some of the latest AI developments from major players like Llama 3.1 405B  GPT-4o and Claude 3.5 Sonnet   Llama 3.1 405B is the first open-source model that performs on par with leading proprietary AI models in general knowledge  steerability  math  tool use  and multilingual translation  among other capabilities.   Meta announced the launch of Llama 3.1  which is the largest open-source AI model to date  and has surpassed OpenAIs GPT-4o and Anthropics Claude 3.5 Sonnet in multiple benchmark tests!   In this step-by-step guide  we will cover what Llama 3.1 405B is  how to use Llama 3.1 405B locally  and why Llama 3.1 405B is so much better than GPT-4o and Claude 3.5 Sonnet.   I highly recommend you watch this video to the end is a game changer in your chatbot that will realize the power of Llama 3.1 405B!   If you like this topic and you want to support me:   Clap my article 50 times; that will really help me out.U+1F44FFollow me on Medium and subscribe to get my latest articleU+1FAF6Follow me on my YouTube channelMore info on my discordLlama 3.1 405B is Metas largest model  trained with over 15 trillion tokens. For this  Meta optimized the entire training stack and trained it on more than 16 000 H100 GPUs  making it the first Llama model trained at this scale.   According to Meta  this version of the original model (Llama 1 and Llama 2) has 128K context length  improved reasoning and coding capabilities. Meta has also upgraded both multilingual 8B and 70B models.   Key Features of Llama 3.1 40 5B:Llama\n",
            "Score\t 0.5368425639023651\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "17Jbo1FH6mjH"
      },
      "outputs": [],
      "source": [
        "query_bundle = hyde(\"Write about Llama 3.1 Model, BERT and PEFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UZEK63K77W7X"
      },
      "outputs": [],
      "source": [
        "hyde_doc = query_bundle.embedding_strs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "wyzwkpSn7Yi1",
        "outputId": "6f48df41-78e0-4d6a-dc5c-c67272667250"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Llama 3.1 model, developed by Meta, represents a significant advancement in the field of natural language processing (NLP). It builds upon the foundation laid by its predecessors, Llama 1 and Llama 2, by incorporating more extensive training data and improved architectural designs. Llama 3.1 is designed to enhance performance in various NLP tasks, such as text generation, summarization, and question-answering, making it a versatile tool for developers and researchers alike.\\n\\nIn contrast, BERT (Bidirectional Encoder Representations from Transformers), introduced by Google in 2018, revolutionized the way models understand context in language. BERT employs a transformer architecture that processes text bidirectionally, allowing it to capture the nuances of language more effectively than previous models that read text in a unidirectional manner. This capability enables BERT to excel in tasks like sentiment analysis, named entity recognition, and other applications requiring a deep understanding of context.\\n\\nAnother important concept in the realm of NLP is PEFT (Parameter-Efficient Fine-Tuning). PEFT techniques allow models like Llama 3.1 and BERT to be fine-tuned on specific tasks with significantly fewer parameters than traditional fine-tuning methods. This approach not only reduces the computational resources required but also minimizes the risk of overfitting, making it easier to adapt large pre-trained models to specialized applications. By leveraging PEFT, practitioners can achieve high performance on niche tasks without the need for extensive retraining, thus democratizing access to advanced NLP capabilities.\\n\\nIn summary, the Llama 3.1 model, BERT, and PEFT represent key innovations in the NLP landscape. Llama 3.1 enhances the capabilities of language models, BERT provides a robust framework for understanding context, and PEFT offers a practical approach to fine-tuning these powerful models for specific applications. Together, they contribute to the ongoing evolution of NLP technologies, enabling more sophisticated and efficient language understanding and generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "hyde_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSHSMLQHmvh0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "llamaindexkernel",
      "language": "python",
      "name": "llamaindexkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f3f20f8cd2f455fba29f8ed87aafa2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_952fe494dc9b4779afe29ca92db7cc83",
              "IPY_MODEL_eaa196f2fb29452c8014dab40fdce3f6",
              "IPY_MODEL_5da3d5c9fec148e693c2dad5cb380cfa"
            ],
            "layout": "IPY_MODEL_7f3774ff6216492c959bfb06cf3ea154"
          }
        },
        "952fe494dc9b4779afe29ca92db7cc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c4e286776574a2c81b09c4c4fd966f1",
            "placeholder": "​",
            "style": "IPY_MODEL_d9aca64c34c54ab189d694f64150ed40",
            "value": "vectorstore.zip: 100%"
          }
        },
        "eaa196f2fb29452c8014dab40fdce3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d98a2764f3d4db08a7225d40dcdcd85",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_855afc704cb7413cb6fd3de5de997b0b",
            "value": 97198458
          }
        },
        "5da3d5c9fec148e693c2dad5cb380cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91eeba72a72841fb9b7b5392cabb46db",
            "placeholder": "​",
            "style": "IPY_MODEL_b7d4d978b2834e02b81318356a0bea2f",
            "value": " 97.2M/97.2M [00:02&lt;00:00, 42.6MB/s]"
          }
        },
        "7f3774ff6216492c959bfb06cf3ea154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4e286776574a2c81b09c4c4fd966f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9aca64c34c54ab189d694f64150ed40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d98a2764f3d4db08a7225d40dcdcd85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855afc704cb7413cb6fd3de5de997b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91eeba72a72841fb9b7b5392cabb46db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d4d978b2834e02b81318356a0bea2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}