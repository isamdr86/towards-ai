{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isamdr86/towards-ai/blob/main/notebooks/GPT_4o_mini_Fine_Tuning_ir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3-67Ng0h6Or"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/GPT_4o_mini_Fine_Tuning.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u26Xu9-_zHyE",
        "outputId": "17935fea-7df9-42e7-cfea-9bf635c89b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.51.2 chromadb==0.5.5 pydantic llama-index-vector-stores-chroma==0.1.10 jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 tiktoken==0.7.0 --force-reinstall"
      ],
      "metadata": {
        "id": "i_dMsXTVycjk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xtfOgVDAzw9p"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6fniKCbv0Gf0"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG_jfHx20Kwh"
      },
      "source": [
        "## Create Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "2d3dba0e64d14f919671e02a85b5b675",
            "d2257c2b668046acb477d325219b454f",
            "a1f6885d9f314ac99ca8b3de514c6956",
            "f67e88b47e214d90984d16dc59fe509e",
            "dacd042b780f42bbb0c14642425f75d7",
            "22265c3bf8304c8dbcb12ec58f528991",
            "8cfce3e5c08145839912b8a2ded2c6f4",
            "a2b2ca415fe142f4878a2d70949dfb1e",
            "fc919e6ae5f74640b03639dbd7b2dd0c",
            "ad9d9133470642eeb9b3b85b426c179a",
            "671ba99566c743c3a30fd802ce6c5e78"
          ]
        },
        "id": "52Q9jvWz0JG1",
        "outputId": "31dac565-eed2-47c9-fd91-a56bd9bd9382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d3dba0e64d14f919671e02a85b5b675"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Downloading Vector store from Hugging face hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvIkYZTq0Sjw",
        "outputId": "af1c0c73-c59a-4d14-f66a-ed2d5198fc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7JKjU64s18XQ"
      },
      "outputs": [],
      "source": [
        "# Setup an Embedding Model\n",
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Default Embedding model\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F2IwHF0c0U_Z"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create your index\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create your index\n",
        "\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgom6yv60x8c"
      },
      "source": [
        "### GPT-4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IJ_AK9bD010r"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm_gpt_4o = OpenAI(temperature=1, model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "kIe78uYz0uGt",
        "outputId": "d7c47cd6-1a2b-4d72-bb99-1f1df3ad56b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"When comparing the knowledge retention abilities of a Retrieval-Augmented Generation (RAG) model to a BERT-based model that has been fine-tuned using PEFT techniques, there are notable differences in their outputs when the knowledge source is removed. A RAG model combines pre-trained parametric and non-parametric memory, allowing it to dynamically retrieve and generate language based on external, dense sources like a Wikipedia index. If the knowledge source is removed, the RAG model's ability to generate accurate, fact-based responses significantly diminishes, since it relies heavily on retrieving up-to-date information from its non-parametric memory.\\n\\nIn contrast, a BERT-based model fine-tuned using PEFT techniques primarily relies on the knowledge embedded within its parameters. This means that even without an external knowledge source, it can continue to generate responses based on the information encoded during training. However, because it lacks the dynamic retrieval component of a RAG model, its responses might not be as specific or current, potentially lacking in-depth factual updates or context-specific insights that are present in RAG-generated outputs.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Query Engine\n",
        "query_engine_0 = vector_index.as_query_engine(llm= llm_gpt_4o , similarity_top_k=5)\n",
        "\n",
        "response_gpt_4o = query_engine_0.query(\"Compare the knowledge retention abilities of a RAG model versus a BERT-based model that has been extensively fine-tuned using PEFT techniques. How do their outputs differ when the knowledge source is removed?\")\n",
        "\n",
        "response_gpt_4o.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVJEEMYO1dWc",
        "outputId": "3cdc7513-4f93-4675-a6fe-c0d7bebfacda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 727ac73b-1bfd-4782-8523-fa38b48cec50\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups\n",
            "Text\t We evaluate the performance of introducing RAGate according to its binary classification performance and the effectiveness of the resulting response generation. Specifically, we use the KE-TOD dataset (Chen et al., 2022), which has fully annotated 5,324 dialogues and 52,063 turns of conversations. In particular, it is associated with 33,761 knowledge snippets to be retrieved and augmented. In addition, KETOD was developed with human labels on turns of conversations (around 12.1% of turns) about the need for augmenting with retrieved knowledge snippets for a natural and informative system response. Hence, we use these human labels as natural ground truths when evaluating RAGate. It is worth indicating that many current knowledge-augmented conversational datasets often ground their conversations on the knowledge snippet, such as Wizard of Wikipedia (Dinan et al., 2018) and CMU_DoG (Zhou et al., 2018), which makes them not a natural fit to be investigated in this study. Due to the limited computational resource availability, we explore the use of Llama-v2-7B and Llama-v2-13B to implement RAGate-prompt andfine-tune Llama-v2-7B for RAGate-PEFT. We implement QLoRA using the PEFT library (Mangrulkar et al., 2022) and set the lower rank to 16. As discussed in Section 3, we have various input features to be combined for performance optimisation. We begin with the use of context only, then concatenate the context with the real response (contx-resp), with the synthetic response and recognised entities (contx-syn-resp-ner) and further extend with the use of retrieved knowledge (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the\n",
            "Score\t 0.48173540422761163\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups', 'tokens': 781, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered using the information given, which are unlikely to be found elsewhere:\\n\\n1. **What dataset is used to evaluate the performance of RAGate, and what specific features does it include for assessing the need for knowledge augmentation in conversational responses?**\\n   - This question targets the specific dataset (KE-TOD) and its features, including the percentage of annotated turns that require knowledge snippets, which is detailed in the context.\\n\\n2. **What methods are employed to retrieve relevant knowledge snippets for augmenting responses in the RAGate framework, and how is their performance evaluated?**\\n   - This question focuses on the retrieval methods (TF-IDF and BERT ranker) and the evaluation metrics (Recall@1 and Recall@3) mentioned in the context, providing insights into the technical aspects of the RAGate implementation.', 'prev_section_summary': \"The section discusses the RAGate-MHA model, which incorporates a multi-head attention mechanism to enhance conversational systems by assessing the necessity of augmentation. It outlines the model's structure, emphasizing the interaction between queries (Q), keys (K), and values (V) in the attention mechanism, defined by the formula Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V. The context and retrieved knowledge are integrated into these roles to evaluate augmentation needs. Additionally, the section references the encoder construction of a transformer model, which includes input embedding and position encoding layers to process the input data effectively. Key topics include multi-head attention, augmentation necessity, and transformer model architecture.\", 'section_summary': \"The section discusses the evaluation of the RAGate framework for conversational systems, focusing on its performance in binary classification and response generation. Key topics include:\\n\\n1. **Dataset**: The KE-TOD dataset, which consists of 5,324 dialogues and 52,063 conversation turns, is used for evaluation. It includes 33,761 knowledge snippets and human annotations indicating that approximately 12.1% of the conversation turns require knowledge augmentation for more informative responses.\\n\\n2. **Knowledge Augmentation**: The study emphasizes the importance of knowledge snippets in enhancing conversational responses, contrasting KE-TOD with other datasets that are not suitable for this investigation.\\n\\n3. **Model Implementation**: The RAGate framework is implemented using Llama-v2-7B and Llama-v2-13B models, with fine-tuning performed on Llama-v2-7B using QLoRA and the PEFT library.\\n\\n4. **Input Features**: Various input features are combined for performance optimization, including context, real responses, synthetic responses, recognized entities, and retrieved knowledge snippets.\\n\\n5. **Retrieval Methods**: Knowledge snippets are retrieved using TF-IDF and a learned BERT ranker, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n6. **Performance Evaluation**: The evaluation focuses on the retrieval performance of knowledge snippets, with the BERT ranker showing superior results, leading to the augmentation of responses with the top 3 relevant knowledge snippets.\\n\\nOverall, the section highlights the methodology and evaluation strategies employed in assessing the RAGate framework's effectiveness in conversational systems.\", 'excerpt_keywords': 'Keywords: RAGate, KE-TOD, knowledge augmentation, conversational systems, TF-IDF, BERT ranker, response generation, binary classification, Llama-v2, performance evaluation'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.47371784994818655\n",
            "Metadata\t {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are the key components of Retrieval-Augmented Generation (RAG) models, and how do they work together to improve performance on knowledge-intensive NLP tasks?\\n\\n2. How does the RAG model differ from traditional pre-trained language models in terms of accessing and manipulating knowledge for language generation?', 'prev_section_summary': \"The section discusses the use of the BLIP-2 model for visual question answering (VQA) tasks. Key topics include:\\n\\n1. **Model Loading**: Instructions on how to load the BLIP-2 model and processor, with a focus on utilizing GPU resources for improved performance.\\n\\n2. **Input Handling**: The process of preparing input data, which includes loading an image and a corresponding question from a dataset.\\n\\n3. **Textual Prompt Format**: The specific format required for the textual prompt when using the model, which is `Question: {} Answer:`.\\n\\n4. **Preprocessing and Model Inference**: Steps to preprocess the image and prompt, pass them through the model, and decode the generated output.\\n\\n5. **Example Output**: An example of the model's output, demonstrating its ability to recognize elements in the image and respond to the question, albeit with some limitations in accuracy.\\n\\nEntities mentioned include the BLIP-2 model, the Salesforce repository for the model, and the use of the `AutoProcessor` and `Blip2ForConditionalGeneration` classes from the Transformers library.\", 'section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to enhance performance on knowledge-intensive natural language processing (NLP) tasks. Key components of RAG include:\\n\\n1. **Retrieval Mechanism**: RAG models retrieve relevant documents from a dense vector index (e.g., Wikipedia) using a pretrained neural retriever.\\n2. **Generation Mechanism**: The retrieved documents are then passed to a seq2seq model, which generates outputs based on the retrieved information.\\n3. **Joint Fine-Tuning**: Both the retriever and seq2seq modules are initialized from pretrained models and fine-tuned together, allowing them to adapt to specific downstream tasks.\\n\\nThe excerpt highlights the limitations of traditional pre-trained language models in accessing and manipulating knowledge, particularly in knowledge-intensive tasks. RAG models address these limitations by combining parametric memory (the seq2seq model) with non-parametric memory (the dense vector index), enabling better knowledge retrieval and generation. The section references the foundational paper on RAG, which explores the model's architecture and its effectiveness in various NLP tasks.\", 'excerpt_keywords': 'Keywords: RAG, Retrieval-Augmented Generation, dense retrieval, sequence-to-sequence, knowledge-intensive NLP, parametric memory, non-parametric memory, fine-tuning, neural retriever, document retrieval'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 9f09a477-cb92-4520-a3fa-cfd9cef07b1d\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups\n",
            "Text\t (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the development of RAGate-MHA, we explore the combinations of 2 to 8 layers, 2 or 4 heads and the embedding size in [64, 128, 256] for the best classification accuracy. We report the precision, recall, F1, Area Under Curve (AUC) and the False Discovery Rate (FDR) as the main measures to show the classification effectiveness. Next, we further deploy the best-performing RAGate gate function to update the KETOD dialogue system (Chen et al., 2022), which uses GPT-2 (Radford et al., 2019) as the backbone model. To highlight the effect of various augmentation setups, we use the context with the gold action without extra prediction as input to KETOD. Then, we compare the resulting performance to the KETOD model without knowledge augmentation and augmenting every system response as baselines. To report the response generation effectiveness, we report how close the response is to the ground truth via BLEU, ROUGE-1/2/L and BERTScores and the confidence score calculated by the minimum probabilities of individual tokens that compose the responses. As argued by Varshney et al. (2023), this calculated confidence score can highly correlate with a language model’s likelihood of generating hallucinated responses. We trained our models and conducted the evaluations on one machine with one NVIDIA 4090 GPU.\n",
            "Score\t 0.4579078122023978\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups', 'tokens': 781, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered using the information given:\\n\\n1. **What retrieval methods were explored for augmenting knowledge in the RAGate-MHA model, and how was their performance evaluated?**\\n   - This question targets the specific retrieval techniques (TF-IDF and BERT ranker) mentioned in the context, as well as the evaluation metrics (Recall@1 and Recall@3) used to assess their performance.\\n\\n2. **What measures were reported to evaluate the classification effectiveness of the RAGate gate function in the KETOD dialogue system?**\\n   - This question focuses on the specific metrics (precision, recall, F1, AUC, and FDR) that were used to demonstrate the classification effectiveness of the RAGate gate function, which is a unique aspect of the study.', 'prev_section_summary': \"The section discusses the evaluation of the RAGate framework for conversational systems, focusing on its performance in binary classification and response generation. Key topics include:\\n\\n1. **Dataset**: The KE-TOD dataset, which consists of 5,324 dialogues and 52,063 conversation turns, is used for evaluation. It includes 33,761 knowledge snippets and human annotations indicating that approximately 12.1% of the conversation turns require knowledge augmentation for more informative responses.\\n\\n2. **Knowledge Augmentation**: The study emphasizes the importance of knowledge snippets in enhancing conversational responses, contrasting KE-TOD with other datasets that are not suitable for this investigation.\\n\\n3. **Model Implementation**: The RAGate framework is implemented using Llama-v2-7B and Llama-v2-13B models, with fine-tuning performed on Llama-v2-7B using QLoRA and the PEFT library.\\n\\n4. **Input Features**: Various input features are combined for performance optimization, including context, real responses, synthetic responses, recognized entities, and retrieved knowledge snippets.\\n\\n5. **Retrieval Methods**: Knowledge snippets are retrieved using TF-IDF and a learned BERT ranker, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n6. **Performance Evaluation**: The evaluation focuses on the retrieval performance of knowledge snippets, with the BERT ranker showing superior results, leading to the augmentation of responses with the top 3 relevant knowledge snippets.\\n\\nOverall, the section highlights the methodology and evaluation strategies employed in assessing the RAGate framework's effectiveness in conversational systems.\", 'section_summary': \"The section discusses the methodologies and evaluation metrics used in the development of the RAGate-MHA model for conversational systems, specifically focusing on knowledge retrieval and classification effectiveness. Key topics include:\\n\\n1. **Retrieval Methods**: The exploration of TF-IDF and a learned BERT ranker for retrieving relevant knowledge snippets, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n2. **Knowledge Augmentation**: The model augments knowledge by utilizing the top 3 relevant snippets retrieved by the BERT ranker.\\n\\n3. **Model Development**: The RAGate-MHA model's architecture is optimized by experimenting with different configurations, including layer counts, head counts, and embedding sizes to achieve the best classification accuracy.\\n\\n4. **Classification Effectiveness**: The effectiveness of the RAGate gate function is assessed using precision, recall, F1 score, Area Under Curve (AUC), and False Discovery Rate (FDR).\\n\\n5. **KETOD Dialogue System**: The RAGate gate function is integrated into the KETOD dialogue system, which uses GPT-2 as its backbone model. The performance of this augmented system is compared against a baseline without knowledge augmentation.\\n\\n6. **Response Generation Evaluation**: The effectiveness of response generation is measured using BLEU, ROUGE-1/2/L, BERTScores, and a confidence score based on token probabilities, which indicates the likelihood of generating accurate responses.\\n\\n7. **Training and Evaluation Setup**: The models were trained and evaluated on a single machine equipped with an NVIDIA 4090 GPU.\\n\\nOverall, the section highlights the integration of retrieval-augmented generation techniques in conversational AI, focusing on performance metrics and system evaluation.\", 'excerpt_keywords': 'Keywords: RAGate-MHA, knowledge retrieval, TF-IDF, BERT ranker, classification metrics, KE-TOD dataset, conversational systems, response generation, BLEU, AUC'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t daa9740c-f77c-4829-a545-c76da1446dae\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Introduction\n",
            "Text\t Table 2 presents a classification accuracy analysis for adaptive augmentation aimed at system response. The terms \"contx\", \"resp\", and \"know\" are used to denote context, initial system response, and retrieved knowledge snippets as inputs, respectively. \"syn-resp\" and \"ner\" refer to additional synthetic response and named entity recognition steps in the model fine-tuning prompts. The variables h, l, and emb are utilized to specify optimal configurations of the number of heads, layers, and embedding size. In reviewing RA-Gate performance alongside LLM prompting versus fine-tuning, the corresponding performance reported in Table 2 indicates that fine-tuning a Llama-2-7B with QLoRA (RA-Gate-PEFT) can significantly outperform the RA-Gate-Prompt approach. Notably, the RA-Gate-PEFT with context-only input, devoid of extra input features and instruction updates, outperforms. A significant performance leap is witnessed by a large margin (e.g., 0.4082 was the highest score compared to 0.1230 F1). This highlights challenges in adaptive knowledge augmentation tasks, which cannot be effectively handled by prompting a generalized pre-trained language model. Larger language models with in-context learning often yield improved performance. However, RA-Gate-PEFT approaches improve precision (0.5203 to 0.6818) but reduce recall (0.3359 to 0.2321). When retrieved knowledge is incorporated into input features, a notable drop in performance is observed across all evaluated aspects, suggesting complexities induced by incorporated knowledge snippets. The exploration of the impact of the knowledge source, such as wikiHow, demonstrates RA-Gate's strengthened prediction and augmentative abilities. Detailed RA-Gate analysis between fine-tuned LLM and MHA classification reveals superior RA-Gate-MHA performance. Table 2 illustrates a wide-margin recall increase using RA-Gate-MHA, with maximum recall of 0.52, although exhibiting lower precision accuracy. When context and initial response are used as inputs, a higher precision is observed but without recall improvement. Hence, the observed trade-off mandates careful RA-Gate strategy consideration. In exploring classification efficiency, Appendix B provides related discussions to validate observed data using RA-Gate for retrieval augmentation predictions.\n",
            "Score\t 0.44553467413297615\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Introduction', 'tokens': 463, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': '1. How does the performance of the RA-Gate-PEFT approach compare to the RA-Gate-Prompt approach in terms of classification accuracy when using context-only input?\\n\\n2. What trade-offs are observed in precision and recall when incorporating retrieved knowledge snippets into the RA-Gate model, and how does this impact the overall performance of the system?', 'prev_section_summary': 'The section discusses the evaluation of RAGate methods for adaptive retrieval-augmented generation (RAG) in conversational systems. It highlights the classification accuracy of these methods, specifically focusing on their performance as assessed using the KETOD dataset, which provides detailed human labels for RAG response generation. The excerpt identifies three variants of the RAGate methods explored in the study: RAGate-Prompt (using LLM prompting), RAGate-PEFT (parameter-efficient fine-tuned LLMs), and RAGate-MHA (a neural classifier with a Multi-Head Attention structure).', 'section_summary': \"The section discusses the performance analysis of the RA-Gate model in the context of adaptive retrieval-augmented generation for conversational systems. Key topics include:\\n\\n1. **Classification Accuracy**: The comparison between two approaches—RA-Gate-PEFT (fine-tuned with QLoRA) and RA-Gate-Prompt—indicates that RA-Gate-PEFT significantly outperforms RA-Gate-Prompt, especially when using context-only input. The highest classification accuracy score for RA-Gate-PEFT is noted as 0.4082, compared to 0.1230 for RA-Gate-Prompt.\\n\\n2. **Input Features**: The analysis distinguishes between different input types: context (contx), initial system response (resp), and retrieved knowledge snippets (know). The impact of these inputs on model performance is emphasized.\\n\\n3. **Precision and Recall Trade-offs**: The incorporation of retrieved knowledge snippets into the RA-Gate model improves precision (from 0.5203 to 0.6818) but reduces recall (from 0.3359 to 0.2321). This trade-off highlights the complexities of adaptive knowledge augmentation tasks.\\n\\n4. **Performance Metrics**: The section references specific performance metrics, including F1 scores, precision, and recall, to illustrate the effectiveness of different configurations and approaches.\\n\\n5. **Model Configurations**: The variables h, l, and emb are mentioned as optimal configurations for the number of heads, layers, and embedding size in the model fine-tuning process.\\n\\n6. **Knowledge Source Impact**: The exploration of knowledge sources, such as wikiHow, is noted to enhance RA-Gate's predictive and augmentative capabilities.\\n\\n7. **RA-Gate-MHA Performance**: A comparison between fine-tuned LLM and multi-head attention (MHA) classification reveals that RA-Gate-MHA exhibits superior recall but lower precision, indicating a need for strategic consideration in model design.\\n\\nOverall, the section emphasizes the importance of careful consideration of input features and model configurations to optimize performance in conversational systems.\", 'excerpt_keywords': 'Keywords: adaptive retrieval, augmented generation, conversational systems, classification accuracy, RA-Gate, fine-tuning, precision, recall, knowledge snippets, LLM prompting'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 40f472bb-265c-438a-bb45-25be7c023dc8\n",
            "Title\t RAG\n",
            "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
            "Score\t 0.4430181179977016\n",
            "Metadata\t {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are the two formulations of RAG models discussed in the context, and how do they differ in terms of passage retrieval during language generation?\\n\\n2. How do RAG models improve upon traditional parametric seq2seq models in terms of language generation, particularly in knowledge-intensive NLP tasks?', 'prev_section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to enhance performance on knowledge-intensive natural language processing (NLP) tasks. Key components of RAG include:\\n\\n1. **Retrieval Mechanism**: RAG models retrieve relevant documents from a dense vector index (e.g., Wikipedia) using a pretrained neural retriever.\\n2. **Generation Mechanism**: The retrieved documents are then passed to a seq2seq model, which generates outputs based on the retrieved information.\\n3. **Joint Fine-Tuning**: Both the retriever and seq2seq modules are initialized from pretrained models and fine-tuned together, allowing them to adapt to specific downstream tasks.\\n\\nThe excerpt highlights the limitations of traditional pre-trained language models in accessing and manipulating knowledge, particularly in knowledge-intensive tasks. RAG models address these limitations by combining parametric memory (the seq2seq model) with non-parametric memory (the dense vector index), enabling better knowledge retrieval and generation. The section references the foundational paper on RAG, which explores the model's architecture and its effectiveness in various NLP tasks.\", 'section_summary': 'The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pre-trained parametric and non-parametric memory for enhanced language generation. Key topics include:\\n\\n1. **RAG Model Formulations**: Two formulations of RAG models are compared:\\n   - One formulation conditions on the same retrieved passages throughout the entire generated sequence.\\n   - The other formulation allows for different passages to be used for each token in the sequence.\\n\\n2. **Improvements Over Traditional Models**: RAG models outperform traditional parametric sequence-to-sequence (seq2seq) models and task-specific retrieve-and-extract architectures, particularly in knowledge-intensive NLP tasks. They generate more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines.\\n\\n3. **Architecture**: RAG models utilize a dense vector index of Wikipedia as non-parametric memory, accessed via a pre-trained neural retriever, while the parametric memory is based on a pre-trained seq2seq model. Both components are fine-tuned jointly to adapt to downstream tasks.\\n\\n4. **Performance**: The models have set state-of-the-art results on three open-domain question-answering tasks.\\n\\nEntities mentioned include:\\n- RAG models\\n- Pre-trained seq2seq models\\n- Dense vector index of Wikipedia\\n- Neural retriever\\n- Knowledge-intensive NLP tasks\\n- Open-domain QA tasks\\n\\nOverall, the section highlights the innovative approach of RAG models in combining retrieval and generation for improved performance in language tasks.', 'excerpt_keywords': 'Keywords: RAG, retrieval-augmented generation, seq2seq models, dense vector index, knowledge-intensive NLP, neural retriever, language generation, fine-tuning, open-domain QA, parametric memory'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response_gpt_4o.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"Metadata\\t\", src.metadata)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJAuFSPi1el8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS9rqJsL2FXs"
      },
      "source": [
        "### GPT-4o-mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ofd1gaLd2IuW"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm_gpt_4o_mini = OpenAI(temperature=1, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "Yoeq2JO42Odz",
        "outputId": "dca80179-6b8e-4ee2-b923-40d309ff68bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The RAG model combines pre-trained parametric memory from a seq2seq model with non-parametric memory accessed from a dense vector index, allowing it to effectively retrieve and utilize knowledge during language generation. In contrast, a BERT-based model that has been extensively fine-tuned using PEFT techniques relies solely on its learned parameters.\\n\\nWhen the knowledge source is removed, the output of the RAG model may deteriorate significantly as it depends on the retrieved information to enhance its responses. Without access to the external knowledge base, the RAG model would primarily rely on its parametric memory, potentially leading to less specific and diverse outputs. \\n\\nOn the other hand, a fine-tuned BERT model might still produce coherent language, but its outputs could lack factual accuracy or specificity, as it cannot access new information beyond its training data. This limitation could result in more generic or less informed responses compared to the adaptive capabilities of the RAG model when it has access to its knowledge sources.\\n\\nOverall, the RAG model maintains a clearer advantage in knowledge retention and utilization due to its ability to retrieve relevant information dynamically, while the fine-tuned BERT may struggle to produce high-quality outputs in the absence of external knowledge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Query Engine\n",
        "query_engine_1 = vector_index.as_query_engine(llm= llm_gpt_4o_mini , similarity_top_k=5)\n",
        "\n",
        "response_gpt_4o_mini = query_engine_1.query(\"Compare the knowledge retention abilities of a RAG model versus a BERT-based model that has been extensively fine-tuned using PEFT techniques. How do their outputs differ when the knowledge source is removed?\")\n",
        "\n",
        "response_gpt_4o_mini.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZBv86WZ2XWw",
        "outputId": "970fef03-9c1c-4980-e352-1743279dc7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 727ac73b-1bfd-4782-8523-fa38b48cec50\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups\n",
            "Text\t We evaluate the performance of introducing RAGate according to its binary classification performance and the effectiveness of the resulting response generation. Specifically, we use the KE-TOD dataset (Chen et al., 2022), which has fully annotated 5,324 dialogues and 52,063 turns of conversations. In particular, it is associated with 33,761 knowledge snippets to be retrieved and augmented. In addition, KETOD was developed with human labels on turns of conversations (around 12.1% of turns) about the need for augmenting with retrieved knowledge snippets for a natural and informative system response. Hence, we use these human labels as natural ground truths when evaluating RAGate. It is worth indicating that many current knowledge-augmented conversational datasets often ground their conversations on the knowledge snippet, such as Wizard of Wikipedia (Dinan et al., 2018) and CMU_DoG (Zhou et al., 2018), which makes them not a natural fit to be investigated in this study. Due to the limited computational resource availability, we explore the use of Llama-v2-7B and Llama-v2-13B to implement RAGate-prompt andfine-tune Llama-v2-7B for RAGate-PEFT. We implement QLoRA using the PEFT library (Mangrulkar et al., 2022) and set the lower rank to 16. As discussed in Section 3, we have various input features to be combined for performance optimisation. We begin with the use of context only, then concatenate the context with the real response (contx-resp), with the synthetic response and recognised entities (contx-syn-resp-ner) and further extend with the use of retrieved knowledge (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the\n",
            "Score\t 0.48173540422761163\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups', 'tokens': 781, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered using the information given, which are unlikely to be found elsewhere:\\n\\n1. **What dataset is used to evaluate the performance of RAGate, and what specific features does it include for assessing the need for knowledge augmentation in conversational responses?**\\n   - This question targets the specific dataset (KE-TOD) and its features, including the percentage of annotated turns that require knowledge snippets, which is detailed in the context.\\n\\n2. **What methods are employed to retrieve relevant knowledge snippets for augmenting responses in the RAGate framework, and how is their performance evaluated?**\\n   - This question focuses on the retrieval methods (TF-IDF and BERT ranker) and the evaluation metrics (Recall@1 and Recall@3) mentioned in the context, providing insights into the technical aspects of the RAGate implementation.', 'prev_section_summary': \"The section discusses the RAGate-MHA model, which incorporates a multi-head attention mechanism to enhance conversational systems by assessing the necessity of augmentation. It outlines the model's structure, emphasizing the interaction between queries (Q), keys (K), and values (V) in the attention mechanism, defined by the formula Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V. The context and retrieved knowledge are integrated into these roles to evaluate augmentation needs. Additionally, the section references the encoder construction of a transformer model, which includes input embedding and position encoding layers to process the input data effectively. Key topics include multi-head attention, augmentation necessity, and transformer model architecture.\", 'section_summary': \"The section discusses the evaluation of the RAGate framework for conversational systems, focusing on its performance in binary classification and response generation. Key topics include:\\n\\n1. **Dataset**: The KE-TOD dataset, which consists of 5,324 dialogues and 52,063 conversation turns, is used for evaluation. It includes 33,761 knowledge snippets and human annotations indicating that approximately 12.1% of the conversation turns require knowledge augmentation for more informative responses.\\n\\n2. **Knowledge Augmentation**: The study emphasizes the importance of knowledge snippets in enhancing conversational responses, contrasting KE-TOD with other datasets that are not suitable for this investigation.\\n\\n3. **Model Implementation**: The RAGate framework is implemented using Llama-v2-7B and Llama-v2-13B models, with fine-tuning performed on Llama-v2-7B using QLoRA and the PEFT library.\\n\\n4. **Input Features**: Various input features are combined for performance optimization, including context, real responses, synthetic responses, recognized entities, and retrieved knowledge snippets.\\n\\n5. **Retrieval Methods**: Knowledge snippets are retrieved using TF-IDF and a learned BERT ranker, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n6. **Performance Evaluation**: The evaluation focuses on the retrieval performance of knowledge snippets, with the BERT ranker showing superior results, leading to the augmentation of responses with the top 3 relevant knowledge snippets.\\n\\nOverall, the section highlights the methodology and evaluation strategies employed in assessing the RAGate framework's effectiveness in conversational systems.\", 'excerpt_keywords': 'Keywords: RAGate, KE-TOD, knowledge augmentation, conversational systems, TF-IDF, BERT ranker, response generation, binary classification, Llama-v2, performance evaluation'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.47371784994818655\n",
            "Metadata\t {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are the key components of Retrieval-Augmented Generation (RAG) models, and how do they work together to improve performance on knowledge-intensive NLP tasks?\\n\\n2. How does the RAG model differ from traditional pre-trained language models in terms of accessing and manipulating knowledge for language generation?', 'prev_section_summary': \"The section discusses the use of the BLIP-2 model for visual question answering (VQA) tasks. Key topics include:\\n\\n1. **Model Loading**: Instructions on how to load the BLIP-2 model and processor, with a focus on utilizing GPU resources for improved performance.\\n\\n2. **Input Handling**: The process of preparing input data, which includes loading an image and a corresponding question from a dataset.\\n\\n3. **Textual Prompt Format**: The specific format required for the textual prompt when using the model, which is `Question: {} Answer:`.\\n\\n4. **Preprocessing and Model Inference**: Steps to preprocess the image and prompt, pass them through the model, and decode the generated output.\\n\\n5. **Example Output**: An example of the model's output, demonstrating its ability to recognize elements in the image and respond to the question, albeit with some limitations in accuracy.\\n\\nEntities mentioned include the BLIP-2 model, the Salesforce repository for the model, and the use of the `AutoProcessor` and `Blip2ForConditionalGeneration` classes from the Transformers library.\", 'section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to enhance performance on knowledge-intensive natural language processing (NLP) tasks. Key components of RAG include:\\n\\n1. **Retrieval Mechanism**: RAG models retrieve relevant documents from a dense vector index (e.g., Wikipedia) using a pretrained neural retriever.\\n2. **Generation Mechanism**: The retrieved documents are then passed to a seq2seq model, which generates outputs based on the retrieved information.\\n3. **Joint Fine-Tuning**: Both the retriever and seq2seq modules are initialized from pretrained models and fine-tuned together, allowing them to adapt to specific downstream tasks.\\n\\nThe excerpt highlights the limitations of traditional pre-trained language models in accessing and manipulating knowledge, particularly in knowledge-intensive tasks. RAG models address these limitations by combining parametric memory (the seq2seq model) with non-parametric memory (the dense vector index), enabling better knowledge retrieval and generation. The section references the foundational paper on RAG, which explores the model's architecture and its effectiveness in various NLP tasks.\", 'excerpt_keywords': 'Keywords: RAG, Retrieval-Augmented Generation, dense retrieval, sequence-to-sequence, knowledge-intensive NLP, parametric memory, non-parametric memory, fine-tuning, neural retriever, document retrieval'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 9f09a477-cb92-4520-a3fa-cfd9cef07b1d\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups\n",
            "Text\t (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the development of RAGate-MHA, we explore the combinations of 2 to 8 layers, 2 or 4 heads and the embedding size in [64, 128, 256] for the best classification accuracy. We report the precision, recall, F1, Area Under Curve (AUC) and the False Discovery Rate (FDR) as the main measures to show the classification effectiveness. Next, we further deploy the best-performing RAGate gate function to update the KETOD dialogue system (Chen et al., 2022), which uses GPT-2 (Radford et al., 2019) as the backbone model. To highlight the effect of various augmentation setups, we use the context with the gold action without extra prediction as input to KETOD. Then, we compare the resulting performance to the KETOD model without knowledge augmentation and augmenting every system response as baselines. To report the response generation effectiveness, we report how close the response is to the ground truth via BLEU, ROUGE-1/2/L and BERTScores and the confidence score calculated by the minimum probabilities of individual tokens that compose the responses. As argued by Varshney et al. (2023), this calculated confidence score can highly correlate with a language model’s likelihood of generating hallucinated responses. We trained our models and conducted the evaluations on one machine with one NVIDIA 4090 GPU.\n",
            "Score\t 0.4579078122023978\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups', 'tokens': 781, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered using the information given:\\n\\n1. **What retrieval methods were explored for augmenting knowledge in the RAGate-MHA model, and how was their performance evaluated?**\\n   - This question targets the specific retrieval techniques (TF-IDF and BERT ranker) mentioned in the context, as well as the evaluation metrics (Recall@1 and Recall@3) used to assess their performance.\\n\\n2. **What measures were reported to evaluate the classification effectiveness of the RAGate gate function in the KETOD dialogue system?**\\n   - This question focuses on the specific metrics (precision, recall, F1, AUC, and FDR) that were used to demonstrate the classification effectiveness of the RAGate gate function, which is a unique aspect of the study.', 'prev_section_summary': \"The section discusses the evaluation of the RAGate framework for conversational systems, focusing on its performance in binary classification and response generation. Key topics include:\\n\\n1. **Dataset**: The KE-TOD dataset, which consists of 5,324 dialogues and 52,063 conversation turns, is used for evaluation. It includes 33,761 knowledge snippets and human annotations indicating that approximately 12.1% of the conversation turns require knowledge augmentation for more informative responses.\\n\\n2. **Knowledge Augmentation**: The study emphasizes the importance of knowledge snippets in enhancing conversational responses, contrasting KE-TOD with other datasets that are not suitable for this investigation.\\n\\n3. **Model Implementation**: The RAGate framework is implemented using Llama-v2-7B and Llama-v2-13B models, with fine-tuning performed on Llama-v2-7B using QLoRA and the PEFT library.\\n\\n4. **Input Features**: Various input features are combined for performance optimization, including context, real responses, synthetic responses, recognized entities, and retrieved knowledge snippets.\\n\\n5. **Retrieval Methods**: Knowledge snippets are retrieved using TF-IDF and a learned BERT ranker, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n6. **Performance Evaluation**: The evaluation focuses on the retrieval performance of knowledge snippets, with the BERT ranker showing superior results, leading to the augmentation of responses with the top 3 relevant knowledge snippets.\\n\\nOverall, the section highlights the methodology and evaluation strategies employed in assessing the RAGate framework's effectiveness in conversational systems.\", 'section_summary': \"The section discusses the methodologies and evaluation metrics used in the development of the RAGate-MHA model for conversational systems, specifically focusing on knowledge retrieval and classification effectiveness. Key topics include:\\n\\n1. **Retrieval Methods**: The exploration of TF-IDF and a learned BERT ranker for retrieving relevant knowledge snippets, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n2. **Knowledge Augmentation**: The model augments knowledge by utilizing the top 3 relevant snippets retrieved by the BERT ranker.\\n\\n3. **Model Development**: The RAGate-MHA model's architecture is optimized by experimenting with different configurations, including layer counts, head counts, and embedding sizes to achieve the best classification accuracy.\\n\\n4. **Classification Effectiveness**: The effectiveness of the RAGate gate function is assessed using precision, recall, F1 score, Area Under Curve (AUC), and False Discovery Rate (FDR).\\n\\n5. **KETOD Dialogue System**: The RAGate gate function is integrated into the KETOD dialogue system, which uses GPT-2 as its backbone model. The performance of this augmented system is compared against a baseline without knowledge augmentation.\\n\\n6. **Response Generation Evaluation**: The effectiveness of response generation is measured using BLEU, ROUGE-1/2/L, BERTScores, and a confidence score based on token probabilities, which indicates the likelihood of generating accurate responses.\\n\\n7. **Training and Evaluation Setup**: The models were trained and evaluated on a single machine equipped with an NVIDIA 4090 GPU.\\n\\nOverall, the section highlights the integration of retrieval-augmented generation techniques in conversational AI, focusing on performance metrics and system evaluation.\", 'excerpt_keywords': 'Keywords: RAGate-MHA, knowledge retrieval, TF-IDF, BERT ranker, classification metrics, KE-TOD dataset, conversational systems, response generation, BLEU, AUC'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t daa9740c-f77c-4829-a545-c76da1446dae\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Introduction\n",
            "Text\t Table 2 presents a classification accuracy analysis for adaptive augmentation aimed at system response. The terms \"contx\", \"resp\", and \"know\" are used to denote context, initial system response, and retrieved knowledge snippets as inputs, respectively. \"syn-resp\" and \"ner\" refer to additional synthetic response and named entity recognition steps in the model fine-tuning prompts. The variables h, l, and emb are utilized to specify optimal configurations of the number of heads, layers, and embedding size. In reviewing RA-Gate performance alongside LLM prompting versus fine-tuning, the corresponding performance reported in Table 2 indicates that fine-tuning a Llama-2-7B with QLoRA (RA-Gate-PEFT) can significantly outperform the RA-Gate-Prompt approach. Notably, the RA-Gate-PEFT with context-only input, devoid of extra input features and instruction updates, outperforms. A significant performance leap is witnessed by a large margin (e.g., 0.4082 was the highest score compared to 0.1230 F1). This highlights challenges in adaptive knowledge augmentation tasks, which cannot be effectively handled by prompting a generalized pre-trained language model. Larger language models with in-context learning often yield improved performance. However, RA-Gate-PEFT approaches improve precision (0.5203 to 0.6818) but reduce recall (0.3359 to 0.2321). When retrieved knowledge is incorporated into input features, a notable drop in performance is observed across all evaluated aspects, suggesting complexities induced by incorporated knowledge snippets. The exploration of the impact of the knowledge source, such as wikiHow, demonstrates RA-Gate's strengthened prediction and augmentative abilities. Detailed RA-Gate analysis between fine-tuned LLM and MHA classification reveals superior RA-Gate-MHA performance. Table 2 illustrates a wide-margin recall increase using RA-Gate-MHA, with maximum recall of 0.52, although exhibiting lower precision accuracy. When context and initial response are used as inputs, a higher precision is observed but without recall improvement. Hence, the observed trade-off mandates careful RA-Gate strategy consideration. In exploring classification efficiency, Appendix B provides related discussions to validate observed data using RA-Gate for retrieval augmentation predictions.\n",
            "Score\t 0.44553467413297615\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Introduction', 'tokens': 463, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': '1. How does the performance of the RA-Gate-PEFT approach compare to the RA-Gate-Prompt approach in terms of classification accuracy when using context-only input?\\n\\n2. What trade-offs are observed in precision and recall when incorporating retrieved knowledge snippets into the RA-Gate model, and how does this impact the overall performance of the system?', 'prev_section_summary': 'The section discusses the evaluation of RAGate methods for adaptive retrieval-augmented generation (RAG) in conversational systems. It highlights the classification accuracy of these methods, specifically focusing on their performance as assessed using the KETOD dataset, which provides detailed human labels for RAG response generation. The excerpt identifies three variants of the RAGate methods explored in the study: RAGate-Prompt (using LLM prompting), RAGate-PEFT (parameter-efficient fine-tuned LLMs), and RAGate-MHA (a neural classifier with a Multi-Head Attention structure).', 'section_summary': \"The section discusses the performance analysis of the RA-Gate model in the context of adaptive retrieval-augmented generation for conversational systems. Key topics include:\\n\\n1. **Classification Accuracy**: The comparison between two approaches—RA-Gate-PEFT (fine-tuned with QLoRA) and RA-Gate-Prompt—indicates that RA-Gate-PEFT significantly outperforms RA-Gate-Prompt, especially when using context-only input. The highest classification accuracy score for RA-Gate-PEFT is noted as 0.4082, compared to 0.1230 for RA-Gate-Prompt.\\n\\n2. **Input Features**: The analysis distinguishes between different input types: context (contx), initial system response (resp), and retrieved knowledge snippets (know). The impact of these inputs on model performance is emphasized.\\n\\n3. **Precision and Recall Trade-offs**: The incorporation of retrieved knowledge snippets into the RA-Gate model improves precision (from 0.5203 to 0.6818) but reduces recall (from 0.3359 to 0.2321). This trade-off highlights the complexities of adaptive knowledge augmentation tasks.\\n\\n4. **Performance Metrics**: The section references specific performance metrics, including F1 scores, precision, and recall, to illustrate the effectiveness of different configurations and approaches.\\n\\n5. **Model Configurations**: The variables h, l, and emb are mentioned as optimal configurations for the number of heads, layers, and embedding size in the model fine-tuning process.\\n\\n6. **Knowledge Source Impact**: The exploration of knowledge sources, such as wikiHow, is noted to enhance RA-Gate's predictive and augmentative capabilities.\\n\\n7. **RA-Gate-MHA Performance**: A comparison between fine-tuned LLM and multi-head attention (MHA) classification reveals that RA-Gate-MHA exhibits superior recall but lower precision, indicating a need for strategic consideration in model design.\\n\\nOverall, the section emphasizes the importance of careful consideration of input features and model configurations to optimize performance in conversational systems.\", 'excerpt_keywords': 'Keywords: adaptive retrieval, augmented generation, conversational systems, classification accuracy, RA-Gate, fine-tuning, precision, recall, knowledge snippets, LLM prompting'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 40f472bb-265c-438a-bb45-25be7c023dc8\n",
            "Title\t RAG\n",
            "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
            "Score\t 0.4430181179977016\n",
            "Metadata\t {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are the two formulations of RAG models discussed in the context, and how do they differ in terms of passage retrieval during language generation?\\n\\n2. How do RAG models improve upon traditional parametric seq2seq models in terms of language generation, particularly in knowledge-intensive NLP tasks?', 'prev_section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to enhance performance on knowledge-intensive natural language processing (NLP) tasks. Key components of RAG include:\\n\\n1. **Retrieval Mechanism**: RAG models retrieve relevant documents from a dense vector index (e.g., Wikipedia) using a pretrained neural retriever.\\n2. **Generation Mechanism**: The retrieved documents are then passed to a seq2seq model, which generates outputs based on the retrieved information.\\n3. **Joint Fine-Tuning**: Both the retriever and seq2seq modules are initialized from pretrained models and fine-tuned together, allowing them to adapt to specific downstream tasks.\\n\\nThe excerpt highlights the limitations of traditional pre-trained language models in accessing and manipulating knowledge, particularly in knowledge-intensive tasks. RAG models address these limitations by combining parametric memory (the seq2seq model) with non-parametric memory (the dense vector index), enabling better knowledge retrieval and generation. The section references the foundational paper on RAG, which explores the model's architecture and its effectiveness in various NLP tasks.\", 'section_summary': 'The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pre-trained parametric and non-parametric memory for enhanced language generation. Key topics include:\\n\\n1. **RAG Model Formulations**: Two formulations of RAG models are compared:\\n   - One formulation conditions on the same retrieved passages throughout the entire generated sequence.\\n   - The other formulation allows for different passages to be used for each token in the sequence.\\n\\n2. **Improvements Over Traditional Models**: RAG models outperform traditional parametric sequence-to-sequence (seq2seq) models and task-specific retrieve-and-extract architectures, particularly in knowledge-intensive NLP tasks. They generate more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines.\\n\\n3. **Architecture**: RAG models utilize a dense vector index of Wikipedia as non-parametric memory, accessed via a pre-trained neural retriever, while the parametric memory is based on a pre-trained seq2seq model. Both components are fine-tuned jointly to adapt to downstream tasks.\\n\\n4. **Performance**: The models have set state-of-the-art results on three open-domain question-answering tasks.\\n\\nEntities mentioned include:\\n- RAG models\\n- Pre-trained seq2seq models\\n- Dense vector index of Wikipedia\\n- Neural retriever\\n- Knowledge-intensive NLP tasks\\n- Open-domain QA tasks\\n\\nOverall, the section highlights the innovative approach of RAG models in combining retrieval and generation for improved performance in language tasks.', 'excerpt_keywords': 'Keywords: RAG, retrieval-augmented generation, seq2seq models, dense vector index, knowledge-intensive NLP, neural retriever, language generation, fine-tuning, open-domain QA, parametric memory'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response_gpt_4o_mini.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"Metadata\\t\", src.metadata)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdr5WHyY7SMU"
      },
      "source": [
        "### Dataset Format validation & Number of tokens in training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iretUMT97R92"
      },
      "outputs": [],
      "source": [
        "# Format error checks\n",
        "\n",
        "# https://cookbook.openai.com/examples/chat_finetuning_data_prep\n",
        "\n",
        "from collections import defaultdict\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "def validate_dataset(output_data):\n",
        "\n",
        "  for ex in output_data:\n",
        "      if not isinstance(ex, dict):\n",
        "          format_errors[\"data_type\"] += 1\n",
        "          continue\n",
        "\n",
        "      messages = ex.get(\"messages\", None)\n",
        "      if not messages:\n",
        "          format_errors[\"missing_messages_list\"] += 1\n",
        "          continue\n",
        "\n",
        "      for message in messages:\n",
        "          if \"role\" not in message or \"content\" not in message:\n",
        "              format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "          if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "              format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "          if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "              format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "          content = message.get(\"content\", None)\n",
        "          function_call = message.get(\"function_call\", None)\n",
        "\n",
        "          if (not content and not function_call) or not isinstance(content, str):\n",
        "              format_errors[\"missing_content\"] += 1\n",
        "\n",
        "      if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "          format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "  if format_errors:\n",
        "      print(\"Found errors:\")\n",
        "      for k, v in format_errors.items():\n",
        "          print(f\"{k}: {v}\")\n",
        "  else:\n",
        "      print(\"\\nNo errors found in the Formatted dataset \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jpWFk33gI7Vy"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def counting_no_tokens(output_data):\n",
        "\n",
        "  tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "\n",
        "  total_tokens = sum(len(tokenizer.encode(\" \".join(message['content'] for message in entry['messages']))) for entry in output_data)\n",
        "\n",
        "  print(f\"Total number of tokens in the Dataset: {total_tokens} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b1ZDGaCoQW0M"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import json\n",
        "import jsonlines\n",
        "from pprint import pprint\n",
        "\n",
        "def dataset_preparation(file_name):\n",
        "    file_path = hf_hub_download(\n",
        "        repo_id=\"jaiganesan/GPT_4o_mini_Fine_tune\",\n",
        "        filename=file_name,\n",
        "        repo_type=\"dataset\",\n",
        "        local_dir=\"/content\"\n",
        "    )\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = [json.loads(line) for line in file]\n",
        "\n",
        "    print(\"Total entries in the dataset:\", len(data))\n",
        "    print(\"-_\"*30)\n",
        "    print(data[4])\n",
        "\n",
        "    output_data = []\n",
        "\n",
        "    for entry in data:\n",
        "        formatted_entry = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"As AI Tutor, answer questions related to AI topics in an in-depth and factual manner.\"},\n",
        "                {\"role\": \"user\", \"content\": entry['question']},\n",
        "                {\"role\": \"assistant\", \"content\": entry['answer']}\n",
        "            ]\n",
        "        }\n",
        "        output_data.append(formatted_entry)\n",
        "\n",
        "    # Validate and analyze the output data\n",
        "    validate_dataset(output_data)\n",
        "    counting_no_tokens(output_data)\n",
        "\n",
        "    print(\"-_\"*30)\n",
        "    print(output_data[4])\n",
        "\n",
        "    base_file_name = os.path.splitext(file_name)[0]\n",
        "    output_file_path = f'formatted_{base_file_name}.jsonl'\n",
        "\n",
        "    with jsonlines.open(output_file_path, mode='w') as writer:\n",
        "        writer.write_all(output_data)\n",
        "\n",
        "    print(f\"\\nFormatted dataset has been saved to {output_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "cd0664da827c4c079ae239b1b0469f2c",
            "e59e9762c8b0475997ee6716259d3a0e",
            "6cf24a4c94114cbe912efced411d5a07",
            "22a8bf67689a49d683b1160950486f65",
            "53561ea3f6c94215ae4fec105fa4a110",
            "d19a888a38514636a6b1825ebb1c9b25",
            "51dbb7fb34ab4d15b86db35a83b1fe99",
            "206e3b22827d45239952fa50a862d294",
            "f74111677cc64cde83fb79d581b6fff1",
            "d547d91c40eb466fab141eb115d4c35b",
            "28282d95dcde462c894e9a42eeb670e7"
          ]
        },
        "collapsed": true,
        "id": "LIQ34EWCp3n1",
        "outputId": "53230d69-c6e3-49bf-9f95-d74c4887c172"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "question_answers_data_100.jsonl:   0%|          | 0.00/276k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd0664da827c4c079ae239b1b0469f2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries in the dataset: 100\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "{'source': 'tai_blog', 'question': 'What are the key advantages of using BiFPN in object detection compared to conventional methods?', 'answer': \"BiFPN, or Bi-directional Feature Pyramid Network, offers several advantages in object detection when compared to conventional methods. It's part of the EfficientDet family of object detectors developed by Google Research and is designed to enhance the efficiency and scalability of object detection models.\\n\\n### Key Advantages of BiFPN:\\n\\n1. **Weighted Feature Fusion:**\\n   Unlike conventional methods that simply sum up input features during feature fusion, BiFPN introduces learnable weights to adjust the importance of different input features. This means that during multi-scale fusion, input features are not merely combined indiscriminately but are weighted according to their relevance, which enhances the accuracy of the fusion process.\\n\\n2. **Bidirectional Information Flow:**\\n   Traditional Feature Pyramid Networks (FPN) typically allow one-way information flow, either bottom-up or top-down. BiFPN, however, combines these approaches by introducing a bi-directional path that incorporates both bottom-up and top-down pathways. This design maximizes the use of high-level and low-level features for better object detection across scales.\\n\\n3. **Efficiency and Scalable Design:**\\n   By incorporating compound scaling and EfficientNet as the backbone, BiFPN allows for efficient scaling up of network dimensions such as depth, width, and resolution. This results in better accuracy with significantly fewer parameters and reduced computation needs (FLOPS) in comparison to previous state-of-the-art detectors.\\n\\n### Comparison with Conventional Methods:\\n\\n- **Conventional Methods:** Traditional object detection methods such as Faster R-CNN, SSD, and YOLO have their own strengths and limitations. For instance, Faster R-CNN is known for its high accuracy but at the cost of speed due to its region proposal network. YOLO, on the other hand, is faster but may struggle with detecting smaller objects due to its single shot approach. Each method applies different strategies, including layered CNNs, feature fusion, and multi-stage processing, but might not handle multi-scale features as effectively as BiFPN.\\n\\n- **Performance:** BiFPN outperforms traditional methods by effectively utilizing available computation resources with a simplified yet potent architecture, allowing real-time uses in applications requiring high precision with a smaller model footprint.\\n\\nIn summary, BiFPN's ability to fuse features more intelligently and its bi-directional design make it a superior choice in scenarios demanding high computational efficiency and scalability, like mobile applications or real-time systems.\\n\\nIf you have deeper questions about any specific aspect of BiFPN or related technologies, feel free to ask!\"}\n",
            "\n",
            "No errors found in the Formatted dataset \n",
            "\n",
            "Total number of tokens in the Dataset: 50425 \n",
            "\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "{'messages': [{'role': 'system', 'content': 'As AI Tutor, answer questions related to AI topics in an in-depth and factual manner.'}, {'role': 'user', 'content': 'What are the key advantages of using BiFPN in object detection compared to conventional methods?'}, {'role': 'assistant', 'content': \"BiFPN, or Bi-directional Feature Pyramid Network, offers several advantages in object detection when compared to conventional methods. It's part of the EfficientDet family of object detectors developed by Google Research and is designed to enhance the efficiency and scalability of object detection models.\\n\\n### Key Advantages of BiFPN:\\n\\n1. **Weighted Feature Fusion:**\\n   Unlike conventional methods that simply sum up input features during feature fusion, BiFPN introduces learnable weights to adjust the importance of different input features. This means that during multi-scale fusion, input features are not merely combined indiscriminately but are weighted according to their relevance, which enhances the accuracy of the fusion process.\\n\\n2. **Bidirectional Information Flow:**\\n   Traditional Feature Pyramid Networks (FPN) typically allow one-way information flow, either bottom-up or top-down. BiFPN, however, combines these approaches by introducing a bi-directional path that incorporates both bottom-up and top-down pathways. This design maximizes the use of high-level and low-level features for better object detection across scales.\\n\\n3. **Efficiency and Scalable Design:**\\n   By incorporating compound scaling and EfficientNet as the backbone, BiFPN allows for efficient scaling up of network dimensions such as depth, width, and resolution. This results in better accuracy with significantly fewer parameters and reduced computation needs (FLOPS) in comparison to previous state-of-the-art detectors.\\n\\n### Comparison with Conventional Methods:\\n\\n- **Conventional Methods:** Traditional object detection methods such as Faster R-CNN, SSD, and YOLO have their own strengths and limitations. For instance, Faster R-CNN is known for its high accuracy but at the cost of speed due to its region proposal network. YOLO, on the other hand, is faster but may struggle with detecting smaller objects due to its single shot approach. Each method applies different strategies, including layered CNNs, feature fusion, and multi-stage processing, but might not handle multi-scale features as effectively as BiFPN.\\n\\n- **Performance:** BiFPN outperforms traditional methods by effectively utilizing available computation resources with a simplified yet potent architecture, allowing real-time uses in applications requiring high precision with a smaller model footprint.\\n\\nIn summary, BiFPN's ability to fuse features more intelligently and its bi-directional design make it a superior choice in scenarios demanding high computational efficiency and scalability, like mobile applications or real-time systems.\\n\\nIf you have deeper questions about any specific aspect of BiFPN or related technologies, feel free to ask!\"}]}\n",
            "\n",
            "Formatted dataset has been saved to formatted_question_answers_data_100.jsonl.\n"
          ]
        }
      ],
      "source": [
        "# Training Dataset\n",
        "dataset_preparation(\"question_answers_data_100.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "8ad59ce4800048259eb604b7a9de5918",
            "dd4bd67bc74549b9a1aefe3ba3dd475d",
            "f41b1259a3124c4fa2d499b5ad9a646e",
            "06346c12fc14494493af114ccc4f697c",
            "0fcaf09fd547450aa7eedb1341e77146",
            "68c81f61c2554d99bbb16d71d17de976",
            "983ed2019aa04d2d9fb674d3f5172e7d",
            "328fac496a3d41fabdf2c34519e27c83",
            "13f692d724cd4fa0bc42689a0aa8c02e",
            "becf7ec56ace43e984a5fd6f6c6ef0c2",
            "787b7405c7864af6bc67ec664d54c4a4"
          ]
        },
        "collapsed": true,
        "id": "ecqVH0tXp3kk",
        "outputId": "d5cbfed3-6beb-4297-d939-b6047d52aa11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "question_answers_data_30.jsonl:   0%|          | 0.00/81.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ad59ce4800048259eb604b7a9de5918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries in the dataset: 30\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "{'source': 'openai_cookbooks', 'question': 'How can creating high-quality evaluations for large language models like GPT-4 improve the stability and reliability of AI applications?', 'answer': \"Creating high-quality evaluations for large language models (LLMs), like GPT-4, significantly enhances the stability and reliability of AI applications. Evaluations serve as a robust mechanism to monitor and assess how well these models perform across various scenarios, ultimately leading to improvements in model robustness and reliability.\\n\\nFirstly, high-quality evaluations can help identify and address areas where models may be underperforming. For instance, systematic evaluations can uncover issues such as drifting performance or deteriorating accuracy over time. By regularly evaluating LLMs against a comprehensive set of benchmarks, developers can detect and correct potential degradation in model performance, as seen with concerns over GPT-4's variations in specific tasks and formatting errors.\\n\\nMoreover, evaluations allow developers to monitor model performance in real-world applications and compare them against alternative solutions. For example, tools like LangSmith enable post-deployment monitoring of LLMs, offering functions to analyze and track model outputs, accuracy, and engagement over time. Such continuous assessment ensures models maintain an acceptable level of performance and reliability and facilitates swift intervention if issues arise.\\n\\nAdditionally, evaluations can assist in optimizing specific applications by providing insights into how models respond to different inputs and contexts. By leveraging structured testing frameworks like those designed to evaluate multi-round conversations or coding tasks, developers can fine-tune models to meet specific application needs better. This iterative process of evaluation and refinement helps in aligning model outputs with desired outcomes more effectively.\\n\\nUltimately, thorough evaluations contribute to the development of LLMs that are more consistent and reliable, thereby offering stability to applications relying on these models. By providing frameworks to continuously test and improve models, developers can ensure higher performance standards, thus enhancing the trustworthiness of AI solutions deployed in various industries.\\n\\nFeel free to ask more questions if you wish to explore any particular aspects further!\"}\n",
            "\n",
            "No errors found in the Formatted dataset \n",
            "\n",
            "Total number of tokens in the Dataset: 14811 \n",
            "\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "{'messages': [{'role': 'system', 'content': 'As AI Tutor, answer questions related to AI topics in an in-depth and factual manner.'}, {'role': 'user', 'content': 'How can creating high-quality evaluations for large language models like GPT-4 improve the stability and reliability of AI applications?'}, {'role': 'assistant', 'content': \"Creating high-quality evaluations for large language models (LLMs), like GPT-4, significantly enhances the stability and reliability of AI applications. Evaluations serve as a robust mechanism to monitor and assess how well these models perform across various scenarios, ultimately leading to improvements in model robustness and reliability.\\n\\nFirstly, high-quality evaluations can help identify and address areas where models may be underperforming. For instance, systematic evaluations can uncover issues such as drifting performance or deteriorating accuracy over time. By regularly evaluating LLMs against a comprehensive set of benchmarks, developers can detect and correct potential degradation in model performance, as seen with concerns over GPT-4's variations in specific tasks and formatting errors.\\n\\nMoreover, evaluations allow developers to monitor model performance in real-world applications and compare them against alternative solutions. For example, tools like LangSmith enable post-deployment monitoring of LLMs, offering functions to analyze and track model outputs, accuracy, and engagement over time. Such continuous assessment ensures models maintain an acceptable level of performance and reliability and facilitates swift intervention if issues arise.\\n\\nAdditionally, evaluations can assist in optimizing specific applications by providing insights into how models respond to different inputs and contexts. By leveraging structured testing frameworks like those designed to evaluate multi-round conversations or coding tasks, developers can fine-tune models to meet specific application needs better. This iterative process of evaluation and refinement helps in aligning model outputs with desired outcomes more effectively.\\n\\nUltimately, thorough evaluations contribute to the development of LLMs that are more consistent and reliable, thereby offering stability to applications relying on these models. By providing frameworks to continuously test and improve models, developers can ensure higher performance standards, thus enhancing the trustworthiness of AI solutions deployed in various industries.\\n\\nFeel free to ask more questions if you wish to explore any particular aspects further!\"}]}\n",
            "\n",
            "Formatted dataset has been saved to formatted_question_answers_data_30.jsonl.\n"
          ]
        }
      ],
      "source": [
        "# Evaluation Dataset\n",
        "dataset_preparation(\"question_answers_data_30.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKm-7AlirgPn"
      },
      "source": [
        "**This Formatted Training and Evaluation Datasets are being used in the OpenAI Models Fine tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVgfxd4Jog5H"
      },
      "source": [
        "**Up until now, we have explored response generation in RAG System using GPT-4o and GPT-4o-mini and Formatting Training data. Moving forward, we will focus on response generation using a newly fine-tuned model. In our lesson, we explored the process of fine-tuning through the OpenAI UI. However, if you want to learn about Fine tuning OpenAI Models using Code, You can explore the code sections in the notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01KnnngWoabW"
      },
      "source": [
        "## Response Generation Using New Fine Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eeH_oG6KoQ36"
      },
      "outputs": [],
      "source": [
        "# Fine Tuned Model\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm_gpt_fine_tuned_model = OpenAI(temperature=0.8, model=\"ft:gpt-4o-mini-2024-07-18:irlearning::Apyu0Syv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "ZLnn76-aoQ1I",
        "outputId": "d45ddf05-192e-4321-9499-4307598613b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The knowledge retention abilities of a RAG model and a BERT-based model fine-tuned with Parameter-Efficient Fine-Tuning (PEFT) techniques differ significantly in their approach and outputs, especially when the knowledge source is removed.\\n\\nRAG models integrate a pre-trained dense retrieval (DPR) mechanism with a sequence-to-sequence (seq2seq) model. This combination allows RAG models to access a dense vector index (like Wikipedia) during generation, enabling them to retrieve and utilize relevant external knowledge dynamically. By merging parametric memory (from the seq2seq model) and non-parametric memory (the dense vector index), RAG models excel in tasks requiring up-to-date and factual information, generating responses that are more specific, diverse, and factually accurate compared to traditional models. When the external knowledge source is removed, RAG models rely solely on the information encoded within their parameters, which can limit their ability to generate factually accurate and specific responses, particularly in knowledge-intensive tasks.\\n\\nOn the other hand, a BERT-based model, particularly one fine-tuned extensively using PEFT techniques, retains knowledge primarily within its parameters. While PEFT allows the model to adapt efficiently to new tasks with fewer additional parameters, it does not inherently provide the same external knowledge access that RAG models enjoy. When the knowledge source is removed, the BERT-based model must rely entirely on the knowledge stored within its learned parameters. This can sometimes lead to limitations in generating accurate responses for questions requiring specific, up-to-date, or external information that the model was not trained on.\\n\\nIn summary, RAG models are designed to leverage external sources effectively, making them superior in generating accurate responses in knowledge-intensive tasks. In contrast, BERT-based models, particularly when fine-tuned with PEFT, may struggle more when external knowledge is absent, as their ability to provide accurate and relevant responses is more reliant on the training data they were initially exposed to.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Query Engine\n",
        "query_engine_2 = vector_index.as_query_engine(llm= llm_gpt_fine_tuned_model , similarity_top_k=5)\n",
        "\n",
        "response_fine_tuned_model = query_engine_2.query(\"Compare the knowledge retention abilities of a RAG model versus a BERT-based model that has been extensively fine-tuned using PEFT techniques. How do their outputs differ when the knowledge source is removed?\")\n",
        "\n",
        "response_fine_tuned_model.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYZION6oQyp",
        "outputId": "11a3c460-1e8a-403d-ea5e-e1be73ccbf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 727ac73b-1bfd-4782-8523-fa38b48cec50\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups\n",
            "Text\t We evaluate the performance of introducing RAGate according to its binary classification performance and the effectiveness of the resulting response generation. Specifically, we use the KE-TOD dataset (Chen et al., 2022), which has fully annotated 5,324 dialogues and 52,063 turns of conversations. In particular, it is associated with 33,761 knowledge snippets to be retrieved and augmented. In addition, KETOD was developed with human labels on turns of conversations (around 12.1% of turns) about the need for augmenting with retrieved knowledge snippets for a natural and informative system response. Hence, we use these human labels as natural ground truths when evaluating RAGate. It is worth indicating that many current knowledge-augmented conversational datasets often ground their conversations on the knowledge snippet, such as Wizard of Wikipedia (Dinan et al., 2018) and CMU_DoG (Zhou et al., 2018), which makes them not a natural fit to be investigated in this study. Due to the limited computational resource availability, we explore the use of Llama-v2-7B and Llama-v2-13B to implement RAGate-prompt andfine-tune Llama-v2-7B for RAGate-PEFT. We implement QLoRA using the PEFT library (Mangrulkar et al., 2022) and set the lower rank to 16. As discussed in Section 3, we have various input features to be combined for performance optimisation. We begin with the use of context only, then concatenate the context with the real response (contx-resp), with the synthetic response and recognised entities (contx-syn-resp-ner) and further extend with the use of retrieved knowledge (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the\n",
            "Score\t 0.48173540422761163\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups', 'tokens': 781, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered using the information given, which are unlikely to be found elsewhere:\\n\\n1. **What dataset is used to evaluate the performance of RAGate, and what specific features does it include for assessing the need for knowledge augmentation in conversational responses?**\\n   - This question targets the specific dataset (KE-TOD) and its features, including the percentage of annotated turns that require knowledge snippets, which is detailed in the context.\\n\\n2. **What methods are employed to retrieve relevant knowledge snippets for augmenting responses in the RAGate framework, and how is their performance evaluated?**\\n   - This question focuses on the retrieval methods (TF-IDF and BERT ranker) and the evaluation metrics (Recall@1 and Recall@3) mentioned in the context, providing insights into the technical aspects of the RAGate implementation.', 'prev_section_summary': \"The section discusses the RAGate-MHA model, which incorporates a multi-head attention mechanism to enhance conversational systems by assessing the necessity of augmentation. It outlines the model's structure, emphasizing the interaction between queries (Q), keys (K), and values (V) in the attention mechanism, defined by the formula Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V. The context and retrieved knowledge are integrated into these roles to evaluate augmentation needs. Additionally, the section references the encoder construction of a transformer model, which includes input embedding and position encoding layers to process the input data effectively. Key topics include multi-head attention, augmentation necessity, and transformer model architecture.\", 'section_summary': \"The section discusses the evaluation of the RAGate framework for conversational systems, focusing on its performance in binary classification and response generation. Key topics include:\\n\\n1. **Dataset**: The KE-TOD dataset, which consists of 5,324 dialogues and 52,063 conversation turns, is used for evaluation. It includes 33,761 knowledge snippets and human annotations indicating that approximately 12.1% of the conversation turns require knowledge augmentation for more informative responses.\\n\\n2. **Knowledge Augmentation**: The study emphasizes the importance of knowledge snippets in enhancing conversational responses, contrasting KE-TOD with other datasets that are not suitable for this investigation.\\n\\n3. **Model Implementation**: The RAGate framework is implemented using Llama-v2-7B and Llama-v2-13B models, with fine-tuning performed on Llama-v2-7B using QLoRA and the PEFT library.\\n\\n4. **Input Features**: Various input features are combined for performance optimization, including context, real responses, synthetic responses, recognized entities, and retrieved knowledge snippets.\\n\\n5. **Retrieval Methods**: Knowledge snippets are retrieved using TF-IDF and a learned BERT ranker, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n6. **Performance Evaluation**: The evaluation focuses on the retrieval performance of knowledge snippets, with the BERT ranker showing superior results, leading to the augmentation of responses with the top 3 relevant knowledge snippets.\\n\\nOverall, the section highlights the methodology and evaluation strategies employed in assessing the RAGate framework's effectiveness in conversational systems.\", 'excerpt_keywords': 'Keywords: RAGate, KE-TOD, knowledge augmentation, conversational systems, TF-IDF, BERT ranker, response generation, binary classification, Llama-v2, performance evaluation'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.47371784994818655\n",
            "Metadata\t {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are the key components of Retrieval-Augmented Generation (RAG) models, and how do they work together to improve performance on knowledge-intensive NLP tasks?\\n\\n2. How does the RAG model differ from traditional pre-trained language models in terms of accessing and manipulating knowledge for language generation?', 'prev_section_summary': \"The section discusses the use of the BLIP-2 model for visual question answering (VQA) tasks. Key topics include:\\n\\n1. **Model Loading**: Instructions on how to load the BLIP-2 model and processor, with a focus on utilizing GPU resources for improved performance.\\n\\n2. **Input Handling**: The process of preparing input data, which includes loading an image and a corresponding question from a dataset.\\n\\n3. **Textual Prompt Format**: The specific format required for the textual prompt when using the model, which is `Question: {} Answer:`.\\n\\n4. **Preprocessing and Model Inference**: Steps to preprocess the image and prompt, pass them through the model, and decode the generated output.\\n\\n5. **Example Output**: An example of the model's output, demonstrating its ability to recognize elements in the image and respond to the question, albeit with some limitations in accuracy.\\n\\nEntities mentioned include the BLIP-2 model, the Salesforce repository for the model, and the use of the `AutoProcessor` and `Blip2ForConditionalGeneration` classes from the Transformers library.\", 'section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to enhance performance on knowledge-intensive natural language processing (NLP) tasks. Key components of RAG include:\\n\\n1. **Retrieval Mechanism**: RAG models retrieve relevant documents from a dense vector index (e.g., Wikipedia) using a pretrained neural retriever.\\n2. **Generation Mechanism**: The retrieved documents are then passed to a seq2seq model, which generates outputs based on the retrieved information.\\n3. **Joint Fine-Tuning**: Both the retriever and seq2seq modules are initialized from pretrained models and fine-tuned together, allowing them to adapt to specific downstream tasks.\\n\\nThe excerpt highlights the limitations of traditional pre-trained language models in accessing and manipulating knowledge, particularly in knowledge-intensive tasks. RAG models address these limitations by combining parametric memory (the seq2seq model) with non-parametric memory (the dense vector index), enabling better knowledge retrieval and generation. The section references the foundational paper on RAG, which explores the model's architecture and its effectiveness in various NLP tasks.\", 'excerpt_keywords': 'Keywords: RAG, Retrieval-Augmented Generation, dense retrieval, sequence-to-sequence, knowledge-intensive NLP, parametric memory, non-parametric memory, fine-tuning, neural retriever, document retrieval'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 9f09a477-cb92-4520-a3fa-cfd9cef07b1d\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups\n",
            "Text\t (contx-syn-resp-ner-know) or the source of knowledge (contx-syn-resp-ner-source). Specifically, we retrieve the relevant knowledge by exploring the use of TF-IDF and a learned BERT ranker. We evaluate their performance with the classic Recall@1 and Recall@3 on the test collection. We use a shallow cutoff because we only use top-relevant knowledge snippets for augmentation. Table 1 shows their retrieval performance. According to the leading performance of BERT-Ranker, we augment knowledge with its retrieved top 3 relevant knowledge snippets (i.e., k = 3). Regarding the development of RAGate-MHA, we explore the combinations of 2 to 8 layers, 2 or 4 heads and the embedding size in [64, 128, 256] for the best classification accuracy. We report the precision, recall, F1, Area Under Curve (AUC) and the False Discovery Rate (FDR) as the main measures to show the classification effectiveness. Next, we further deploy the best-performing RAGate gate function to update the KETOD dialogue system (Chen et al., 2022), which uses GPT-2 (Radford et al., 2019) as the backbone model. To highlight the effect of various augmentation setups, we use the context with the gold action without extra prediction as input to KETOD. Then, we compare the resulting performance to the KETOD model without knowledge augmentation and augmenting every system response as baselines. To report the response generation effectiveness, we report how close the response is to the ground truth via BLEU, ROUGE-1/2/L and BERTScores and the confidence score calculated by the minimum probabilities of individual tokens that compose the responses. As argued by Varshney et al. (2023), this calculated confidence score can highly correlate with a language model’s likelihood of generating hallucinated responses. We trained our models and conducted the evaluations on one machine with one NVIDIA 4090 GPU.\n",
            "Score\t 0.4579078122023978\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Model Training and Evaluation Setups', 'tokens': 781, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered using the information given:\\n\\n1. **What retrieval methods were explored for augmenting knowledge in the RAGate-MHA model, and how was their performance evaluated?**\\n   - This question targets the specific retrieval techniques (TF-IDF and BERT ranker) mentioned in the context, as well as the evaluation metrics (Recall@1 and Recall@3) used to assess their performance.\\n\\n2. **What measures were reported to evaluate the classification effectiveness of the RAGate gate function in the KETOD dialogue system?**\\n   - This question focuses on the specific metrics (precision, recall, F1, AUC, and FDR) that were used to demonstrate the classification effectiveness of the RAGate gate function, which is a unique aspect of the study.', 'prev_section_summary': \"The section discusses the evaluation of the RAGate framework for conversational systems, focusing on its performance in binary classification and response generation. Key topics include:\\n\\n1. **Dataset**: The KE-TOD dataset, which consists of 5,324 dialogues and 52,063 conversation turns, is used for evaluation. It includes 33,761 knowledge snippets and human annotations indicating that approximately 12.1% of the conversation turns require knowledge augmentation for more informative responses.\\n\\n2. **Knowledge Augmentation**: The study emphasizes the importance of knowledge snippets in enhancing conversational responses, contrasting KE-TOD with other datasets that are not suitable for this investigation.\\n\\n3. **Model Implementation**: The RAGate framework is implemented using Llama-v2-7B and Llama-v2-13B models, with fine-tuning performed on Llama-v2-7B using QLoRA and the PEFT library.\\n\\n4. **Input Features**: Various input features are combined for performance optimization, including context, real responses, synthetic responses, recognized entities, and retrieved knowledge snippets.\\n\\n5. **Retrieval Methods**: Knowledge snippets are retrieved using TF-IDF and a learned BERT ranker, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n6. **Performance Evaluation**: The evaluation focuses on the retrieval performance of knowledge snippets, with the BERT ranker showing superior results, leading to the augmentation of responses with the top 3 relevant knowledge snippets.\\n\\nOverall, the section highlights the methodology and evaluation strategies employed in assessing the RAGate framework's effectiveness in conversational systems.\", 'section_summary': \"The section discusses the methodologies and evaluation metrics used in the development of the RAGate-MHA model for conversational systems, specifically focusing on knowledge retrieval and classification effectiveness. Key topics include:\\n\\n1. **Retrieval Methods**: The exploration of TF-IDF and a learned BERT ranker for retrieving relevant knowledge snippets, with performance evaluated using Recall@1 and Recall@3 metrics.\\n\\n2. **Knowledge Augmentation**: The model augments knowledge by utilizing the top 3 relevant snippets retrieved by the BERT ranker.\\n\\n3. **Model Development**: The RAGate-MHA model's architecture is optimized by experimenting with different configurations, including layer counts, head counts, and embedding sizes to achieve the best classification accuracy.\\n\\n4. **Classification Effectiveness**: The effectiveness of the RAGate gate function is assessed using precision, recall, F1 score, Area Under Curve (AUC), and False Discovery Rate (FDR).\\n\\n5. **KETOD Dialogue System**: The RAGate gate function is integrated into the KETOD dialogue system, which uses GPT-2 as its backbone model. The performance of this augmented system is compared against a baseline without knowledge augmentation.\\n\\n6. **Response Generation Evaluation**: The effectiveness of response generation is measured using BLEU, ROUGE-1/2/L, BERTScores, and a confidence score based on token probabilities, which indicates the likelihood of generating accurate responses.\\n\\n7. **Training and Evaluation Setup**: The models were trained and evaluated on a single machine equipped with an NVIDIA 4090 GPU.\\n\\nOverall, the section highlights the integration of retrieval-augmented generation techniques in conversational AI, focusing on performance metrics and system evaluation.\", 'excerpt_keywords': 'Keywords: RAGate-MHA, knowledge retrieval, TF-IDF, BERT ranker, classification metrics, KE-TOD dataset, conversational systems, response generation, BLEU, AUC'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t daa9740c-f77c-4829-a545-c76da1446dae\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Introduction\n",
            "Text\t Table 2 presents a classification accuracy analysis for adaptive augmentation aimed at system response. The terms \"contx\", \"resp\", and \"know\" are used to denote context, initial system response, and retrieved knowledge snippets as inputs, respectively. \"syn-resp\" and \"ner\" refer to additional synthetic response and named entity recognition steps in the model fine-tuning prompts. The variables h, l, and emb are utilized to specify optimal configurations of the number of heads, layers, and embedding size. In reviewing RA-Gate performance alongside LLM prompting versus fine-tuning, the corresponding performance reported in Table 2 indicates that fine-tuning a Llama-2-7B with QLoRA (RA-Gate-PEFT) can significantly outperform the RA-Gate-Prompt approach. Notably, the RA-Gate-PEFT with context-only input, devoid of extra input features and instruction updates, outperforms. A significant performance leap is witnessed by a large margin (e.g., 0.4082 was the highest score compared to 0.1230 F1). This highlights challenges in adaptive knowledge augmentation tasks, which cannot be effectively handled by prompting a generalized pre-trained language model. Larger language models with in-context learning often yield improved performance. However, RA-Gate-PEFT approaches improve precision (0.5203 to 0.6818) but reduce recall (0.3359 to 0.2321). When retrieved knowledge is incorporated into input features, a notable drop in performance is observed across all evaluated aspects, suggesting complexities induced by incorporated knowledge snippets. The exploration of the impact of the knowledge source, such as wikiHow, demonstrates RA-Gate's strengthened prediction and augmentative abilities. Detailed RA-Gate analysis between fine-tuned LLM and MHA classification reveals superior RA-Gate-MHA performance. Table 2 illustrates a wide-margin recall increase using RA-Gate-MHA, with maximum recall of 0.52, although exhibiting lower precision accuracy. When context and initial response are used as inputs, a higher precision is observed but without recall improvement. Hence, the observed trade-off mandates careful RA-Gate strategy consideration. In exploring classification efficiency, Appendix B provides related discussions to validate observed data using RA-Gate for retrieval augmentation predictions.\n",
            "Score\t 0.44553467413297615\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Introduction', 'tokens': 463, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': '1. How does the performance of the RA-Gate-PEFT approach compare to the RA-Gate-Prompt approach in terms of classification accuracy when using context-only input?\\n\\n2. What trade-offs are observed in precision and recall when incorporating retrieved knowledge snippets into the RA-Gate model, and how does this impact the overall performance of the system?', 'prev_section_summary': 'The section discusses the evaluation of RAGate methods for adaptive retrieval-augmented generation (RAG) in conversational systems. It highlights the classification accuracy of these methods, specifically focusing on their performance as assessed using the KETOD dataset, which provides detailed human labels for RAG response generation. The excerpt identifies three variants of the RAGate methods explored in the study: RAGate-Prompt (using LLM prompting), RAGate-PEFT (parameter-efficient fine-tuned LLMs), and RAGate-MHA (a neural classifier with a Multi-Head Attention structure).', 'section_summary': \"The section discusses the performance analysis of the RA-Gate model in the context of adaptive retrieval-augmented generation for conversational systems. Key topics include:\\n\\n1. **Classification Accuracy**: The comparison between two approaches—RA-Gate-PEFT (fine-tuned with QLoRA) and RA-Gate-Prompt—indicates that RA-Gate-PEFT significantly outperforms RA-Gate-Prompt, especially when using context-only input. The highest classification accuracy score for RA-Gate-PEFT is noted as 0.4082, compared to 0.1230 for RA-Gate-Prompt.\\n\\n2. **Input Features**: The analysis distinguishes between different input types: context (contx), initial system response (resp), and retrieved knowledge snippets (know). The impact of these inputs on model performance is emphasized.\\n\\n3. **Precision and Recall Trade-offs**: The incorporation of retrieved knowledge snippets into the RA-Gate model improves precision (from 0.5203 to 0.6818) but reduces recall (from 0.3359 to 0.2321). This trade-off highlights the complexities of adaptive knowledge augmentation tasks.\\n\\n4. **Performance Metrics**: The section references specific performance metrics, including F1 scores, precision, and recall, to illustrate the effectiveness of different configurations and approaches.\\n\\n5. **Model Configurations**: The variables h, l, and emb are mentioned as optimal configurations for the number of heads, layers, and embedding size in the model fine-tuning process.\\n\\n6. **Knowledge Source Impact**: The exploration of knowledge sources, such as wikiHow, is noted to enhance RA-Gate's predictive and augmentative capabilities.\\n\\n7. **RA-Gate-MHA Performance**: A comparison between fine-tuned LLM and multi-head attention (MHA) classification reveals that RA-Gate-MHA exhibits superior recall but lower precision, indicating a need for strategic consideration in model design.\\n\\nOverall, the section emphasizes the importance of careful consideration of input features and model configurations to optimize performance in conversational systems.\", 'excerpt_keywords': 'Keywords: adaptive retrieval, augmented generation, conversational systems, classification accuracy, RA-Gate, fine-tuning, precision, recall, knowledge snippets, LLM prompting'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 40f472bb-265c-438a-bb45-25be7c023dc8\n",
            "Title\t RAG\n",
            "Text\t extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generationtasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-artparametric-only seq2seq baseline.*This model was contributed by [ola13](https://huggingface.co/ola13).## Usage tipsRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.## RagConfig[[autodoc]] RagConfig## RagTokenizer[[autodoc]] RagTokenizer## Rag specific outputs[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput## RagRetriever[[autodoc]] RagRetriever<frameworkcontent><pt>## RagModel[[autodoc]] RagModel    - forward## RagSequenceForGeneration[[autodoc]] RagSequenceForGeneration    - forward    - generate## RagTokenForGeneration[[autodoc]] RagTokenForGeneration    - forward    - generate</pt><tf>## TFRagModel[[autodoc]] TFRagModel    - call## TFRagSequenceForGeneration[[autodoc]] TFRagSequenceForGeneration    - call\n",
            "Score\t 0.4430181179977016\n",
            "Metadata\t {'url': 'https://huggingface.co/docs/transformers/model_doc/rag', 'title': 'RAG', 'tokens': 933, 'source': 'transformers', 'questions_this_excerpt_can_answer': '1. What are the two formulations of RAG models discussed in the context, and how do they differ in terms of passage retrieval during language generation?\\n\\n2. How do RAG models improve upon traditional parametric seq2seq models in terms of language generation, particularly in knowledge-intensive NLP tasks?', 'prev_section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to enhance performance on knowledge-intensive natural language processing (NLP) tasks. Key components of RAG include:\\n\\n1. **Retrieval Mechanism**: RAG models retrieve relevant documents from a dense vector index (e.g., Wikipedia) using a pretrained neural retriever.\\n2. **Generation Mechanism**: The retrieved documents are then passed to a seq2seq model, which generates outputs based on the retrieved information.\\n3. **Joint Fine-Tuning**: Both the retriever and seq2seq modules are initialized from pretrained models and fine-tuned together, allowing them to adapt to specific downstream tasks.\\n\\nThe excerpt highlights the limitations of traditional pre-trained language models in accessing and manipulating knowledge, particularly in knowledge-intensive tasks. RAG models address these limitations by combining parametric memory (the seq2seq model) with non-parametric memory (the dense vector index), enabling better knowledge retrieval and generation. The section references the foundational paper on RAG, which explores the model's architecture and its effectiveness in various NLP tasks.\", 'section_summary': 'The section discusses Retrieval-Augmented Generation (RAG) models, which integrate pre-trained parametric and non-parametric memory for enhanced language generation. Key topics include:\\n\\n1. **RAG Model Formulations**: Two formulations of RAG models are compared:\\n   - One formulation conditions on the same retrieved passages throughout the entire generated sequence.\\n   - The other formulation allows for different passages to be used for each token in the sequence.\\n\\n2. **Improvements Over Traditional Models**: RAG models outperform traditional parametric sequence-to-sequence (seq2seq) models and task-specific retrieve-and-extract architectures, particularly in knowledge-intensive NLP tasks. They generate more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines.\\n\\n3. **Architecture**: RAG models utilize a dense vector index of Wikipedia as non-parametric memory, accessed via a pre-trained neural retriever, while the parametric memory is based on a pre-trained seq2seq model. Both components are fine-tuned jointly to adapt to downstream tasks.\\n\\n4. **Performance**: The models have set state-of-the-art results on three open-domain question-answering tasks.\\n\\nEntities mentioned include:\\n- RAG models\\n- Pre-trained seq2seq models\\n- Dense vector index of Wikipedia\\n- Neural retriever\\n- Knowledge-intensive NLP tasks\\n- Open-domain QA tasks\\n\\nOverall, the section highlights the innovative approach of RAG models in combining retrieval and generation for improved performance in language tasks.', 'excerpt_keywords': 'Keywords: RAG, retrieval-augmented generation, seq2seq models, dense vector index, knowledge-intensive NLP, neural retriever, language generation, fine-tuning, open-domain QA, parametric memory'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in response_fine_tuned_model.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"Metadata\\t\", src.metadata)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p3iWDxXxzBN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qf4QFayot91"
      },
      "source": [
        "# Fine Tuning OpenAI Models Using OpenAI API (Code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyEIgPw5kB_"
      },
      "source": [
        "### Upload file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8ROsKdZ25f5x"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "fine_tune_file = client.files.create(\n",
        "    file=open(\"formatted_question_answers_data_100.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6mM5Ns2Asd2",
        "outputId": "d326994b-23c5-4cab-b68c-3f715b42167f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FileObject(id='file-VLM277YEE9fFFg64ZAnd4c', bytes=291689, created_at=1736951701, filename='formatted_question_answers_data_100.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
          ]
        }
      ],
      "source": [
        "pprint(fine_tune_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M3rzu895f22",
        "outputId": "91fd4ed3-3b06-4ab4-f8ce-370f1c40fa66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'file-VLM277YEE9fFFg64ZAnd4c'\n"
          ]
        }
      ],
      "source": [
        "param_training_file_name = fine_tune_file.id\n",
        "pprint(param_training_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyxzpFPi6o-H"
      },
      "source": [
        "### Create Fine tune model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XsGRmaH05fz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07dd854-82cc-4784-8387-b6ef18383b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-wcNzlUfqWDW8juVhSHkCe2Dp', created_at=1736951712, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=2, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-cqPQ00bXAHMrGkKBUoiDmZIi', result_files=[], seed=217695527, status='validating_files', trained_tokens=None, training_file='file-VLM277YEE9fFFg64ZAnd4c', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None, method={'type': 'supervised', 'supervised': {'hyperparameters': {'batch_size': 'auto', 'learning_rate_multiplier': 'auto', 'n_epochs': 2}}})\n"
          ]
        }
      ],
      "source": [
        "result_job = client.fine_tuning.jobs.create(\n",
        "    training_file=param_training_file_name,\n",
        "    model=\"gpt-4o-mini-2024-07-18\",\n",
        "    hyperparameters={ \"n_epochs\":2 }\n",
        ")\n",
        "\n",
        "pprint(result_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1qLwQ1Cv6wjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387477e1-c43e-42ca-d073-a96f83d1dc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ftjob-wcNzlUfqWDW8juVhSHkCe2Dp'\n"
          ]
        }
      ],
      "source": [
        "param_file_tune_job_id = result_job.id\n",
        "pprint(param_file_tune_job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y0X_JtpKQ3E"
      },
      "source": [
        "### Model Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iKeR0YMm6wXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cec067-f919-4eed-f3e8-844bc5aa7373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-wcNzlUfqWDW8juVhSHkCe2Dp', created_at=1736951712, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=2, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-cqPQ00bXAHMrGkKBUoiDmZIi', result_files=[], seed=217695527, status='validating_files', trained_tokens=None, training_file='file-VLM277YEE9fFFg64ZAnd4c', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None, method={'type': 'supervised', 'supervised': {'hyperparameters': {'n_epochs': 2, 'batch_size': 'auto', 'learning_rate_multiplier': 'auto'}}})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Retrieve the state of a fine-tune\n",
        "client.fine_tuning.jobs.retrieve(param_file_tune_job_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9IQwsfQj62vU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0781714-c755-4efa-db79-d36376a7fae6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'validating_files'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Retrieve the status of a fine-tune\n",
        "client.fine_tuning.jobs.retrieve(param_file_tune_job_id).status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mPbk02BWDoU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8185be8-1713-4bee-dac0-12f180c7dce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------ Job Status: validating_files --------------\n",
            "------------ Job Status: validating_files --------------\n",
            "------------ Job Status: validating_files --------------\n",
            "------------ Job Status: validating_files --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: running --------------\n",
            "------------ Job Status: succeeded --------------\n",
            "Job Completed. Detailed Events list:\n",
            "2025-01-15 14:43:27 The job has successfully completed\n",
            "2025-01-15 14:43:21 New fine-tuned model created\n",
            "2025-01-15 14:43:21 Checkpoint created at step 100\n",
            "2025-01-15 14:42:58 Step 200/200: training loss=1.41\n",
            "2025-01-15 14:42:58 Step 199/200: training loss=1.14\n",
            "2025-01-15 14:42:58 Step 198/200: training loss=0.96\n",
            "2025-01-15 14:42:56 Step 197/200: training loss=1.37\n",
            "2025-01-15 14:42:56 Step 196/200: training loss=1.10\n",
            "2025-01-15 14:42:53 Step 195/200: training loss=1.49\n",
            "2025-01-15 14:42:53 Step 194/200: training loss=0.96\n",
            "2025-01-15 14:42:53 Step 193/200: training loss=0.88\n",
            "2025-01-15 14:42:51 Step 192/200: training loss=0.75\n",
            "2025-01-15 14:42:51 Step 191/200: training loss=0.84\n",
            "2025-01-15 14:42:50 Step 190/200: training loss=1.04\n",
            "2025-01-15 14:42:48 Step 189/200: training loss=0.94\n",
            "2025-01-15 14:42:48 Step 188/200: training loss=1.13\n",
            "2025-01-15 14:42:48 Step 187/200: training loss=1.36\n",
            "2025-01-15 14:42:45 Step 186/200: training loss=1.14\n",
            "2025-01-15 14:42:45 Step 185/200: training loss=1.06\n",
            "2025-01-15 14:42:45 Step 184/200: training loss=1.62\n",
            "2025-01-15 14:42:43 Step 183/200: training loss=1.64\n",
            "2025-01-15 14:42:43 Step 182/200: training loss=1.13\n",
            "2025-01-15 14:42:40 Step 181/200: training loss=0.85\n",
            "2025-01-15 14:42:40 Step 180/200: training loss=1.58\n",
            "2025-01-15 14:42:40 Step 179/200: training loss=1.53\n",
            "2025-01-15 14:42:38 Step 178/200: training loss=0.97\n",
            "2025-01-15 14:42:38 Step 177/200: training loss=1.40\n",
            "2025-01-15 14:42:38 Step 176/200: training loss=1.25\n",
            "2025-01-15 14:42:35 Step 175/200: training loss=1.10\n",
            "2025-01-15 14:42:35 Step 174/200: training loss=1.33\n",
            "2025-01-15 14:42:35 Step 173/200: training loss=1.20\n",
            "2025-01-15 14:42:32 Step 172/200: training loss=1.35\n",
            "2025-01-15 14:42:32 Step 171/200: training loss=1.16\n",
            "2025-01-15 14:42:32 Step 170/200: training loss=1.17\n",
            "2025-01-15 14:42:30 Step 169/200: training loss=1.15\n",
            "2025-01-15 14:42:30 Step 168/200: training loss=1.07\n",
            "2025-01-15 14:42:30 Step 167/200: training loss=0.82\n",
            "2025-01-15 14:42:27 Step 166/200: training loss=1.12\n",
            "2025-01-15 14:42:27 Step 165/200: training loss=0.98\n",
            "2025-01-15 14:42:27 Step 164/200: training loss=0.97\n",
            "2025-01-15 14:42:25 Step 163/200: training loss=1.50\n",
            "2025-01-15 14:42:25 Step 162/200: training loss=1.40\n",
            "2025-01-15 14:42:25 Step 161/200: training loss=1.37\n",
            "2025-01-15 14:42:22 Step 160/200: training loss=1.64\n",
            "2025-01-15 14:42:22 Step 159/200: training loss=1.05\n",
            "2025-01-15 14:42:22 Step 158/200: training loss=1.03\n",
            "2025-01-15 14:42:19 Step 157/200: training loss=1.16\n",
            "2025-01-15 14:42:19 Step 156/200: training loss=1.00\n",
            "2025-01-15 14:42:17 Step 155/200: training loss=1.36\n",
            "2025-01-15 14:42:17 Step 154/200: training loss=1.06\n",
            "2025-01-15 14:42:17 Step 153/200: training loss=1.13\n",
            "2025-01-15 14:42:14 Step 152/200: training loss=1.32\n",
            "2025-01-15 14:42:14 Step 151/200: training loss=1.12\n",
            "2025-01-15 14:42:12 Step 150/200: training loss=1.95\n",
            "2025-01-15 14:42:12 Step 149/200: training loss=1.12\n",
            "2025-01-15 14:42:12 Step 148/200: training loss=0.90\n",
            "2025-01-15 14:42:09 Step 147/200: training loss=0.81\n",
            "2025-01-15 14:42:09 Step 146/200: training loss=1.24\n",
            "2025-01-15 14:42:09 Step 145/200: training loss=1.12\n",
            "2025-01-15 14:42:07 Step 144/200: training loss=1.02\n",
            "2025-01-15 14:42:07 Step 143/200: training loss=0.96\n",
            "2025-01-15 14:42:07 Step 142/200: training loss=0.95\n",
            "2025-01-15 14:42:04 Step 141/200: training loss=0.94\n",
            "2025-01-15 14:42:04 Step 140/200: training loss=0.93\n",
            "2025-01-15 14:42:04 Step 139/200: training loss=1.44\n",
            "2025-01-15 14:42:01 Step 138/200: training loss=1.57\n",
            "2025-01-15 14:42:01 Step 137/200: training loss=1.36\n",
            "2025-01-15 14:42:01 Step 136/200: training loss=1.08\n",
            "2025-01-15 14:41:59 Step 135/200: training loss=0.69\n",
            "2025-01-15 14:41:59 Step 134/200: training loss=1.82\n",
            "2025-01-15 14:41:59 Step 133/200: training loss=1.16\n",
            "2025-01-15 14:41:56 Step 132/200: training loss=1.15\n",
            "2025-01-15 14:41:56 Step 131/200: training loss=1.51\n",
            "2025-01-15 14:41:56 Step 130/200: training loss=0.83\n",
            "2025-01-15 14:41:54 Step 129/200: training loss=1.15\n",
            "2025-01-15 14:41:54 Step 128/200: training loss=1.24\n",
            "2025-01-15 14:41:53 Step 127/200: training loss=1.04\n",
            "2025-01-15 14:41:51 Step 126/200: training loss=1.46\n",
            "2025-01-15 14:41:51 Step 125/200: training loss=1.21\n",
            "2025-01-15 14:41:48 Step 124/200: training loss=1.07\n",
            "2025-01-15 14:41:48 Step 123/200: training loss=1.00\n",
            "2025-01-15 14:41:48 Step 122/200: training loss=1.00\n",
            "2025-01-15 14:41:46 Step 121/200: training loss=1.31\n",
            "2025-01-15 14:41:46 Step 120/200: training loss=1.24\n",
            "2025-01-15 14:41:46 Step 119/200: training loss=1.12\n",
            "2025-01-15 14:41:43 Step 118/200: training loss=1.34\n",
            "2025-01-15 14:41:43 Step 117/200: training loss=1.33\n",
            "2025-01-15 14:41:41 Step 116/200: training loss=1.26\n",
            "2025-01-15 14:41:41 Step 115/200: training loss=1.23\n",
            "2025-01-15 14:41:41 Step 114/200: training loss=1.01\n",
            "2025-01-15 14:41:38 Step 113/200: training loss=1.38\n",
            "2025-01-15 14:41:38 Step 112/200: training loss=1.18\n",
            "2025-01-15 14:41:38 Step 111/200: training loss=0.75\n",
            "2025-01-15 14:41:36 Step 110/200: training loss=1.55\n",
            "2025-01-15 14:41:36 Step 109/200: training loss=1.23\n",
            "2025-01-15 14:41:35 Step 108/200: training loss=0.94\n",
            "2025-01-15 14:41:33 Step 107/200: training loss=0.69\n",
            "2025-01-15 14:41:33 Step 106/200: training loss=1.43\n",
            "2025-01-15 14:41:33 Step 105/200: training loss=1.05\n",
            "2025-01-15 14:41:30 Step 104/200: training loss=1.06\n",
            "2025-01-15 14:41:30 Step 103/200: training loss=1.07\n",
            "2025-01-15 14:41:30 Step 102/200: training loss=1.17\n",
            "2025-01-15 14:41:28 Step 101/200: training loss=0.93\n",
            "2025-01-15 14:41:15 Step 100/200: training loss=1.29\n",
            "2025-01-15 14:41:12 Step 99/200: training loss=1.70\n",
            "2025-01-15 14:41:12 Step 98/200: training loss=1.64\n",
            "2025-01-15 14:41:12 Step 97/200: training loss=1.07\n",
            "2025-01-15 14:41:10 Step 96/200: training loss=1.27\n",
            "2025-01-15 14:41:10 Step 95/200: training loss=1.11\n",
            "2025-01-15 14:41:10 Step 94/200: training loss=1.52\n",
            "2025-01-15 14:41:07 Step 93/200: training loss=1.55\n",
            "2025-01-15 14:41:07 Step 92/200: training loss=1.87\n",
            "2025-01-15 14:41:05 Step 91/200: training loss=1.29\n",
            "2025-01-15 14:41:05 Step 90/200: training loss=1.32\n",
            "2025-01-15 14:41:05 Step 89/200: training loss=1.54\n",
            "2025-01-15 14:41:02 Step 88/200: training loss=1.12\n",
            "2025-01-15 14:41:02 Step 87/200: training loss=1.27\n",
            "2025-01-15 14:41:02 Step 86/200: training loss=1.37\n",
            "2025-01-15 14:40:59 Step 85/200: training loss=0.98\n",
            "2025-01-15 14:40:59 Step 84/200: training loss=1.34\n",
            "2025-01-15 14:40:57 Step 83/200: training loss=1.30\n",
            "2025-01-15 14:40:54 Step 82/200: training loss=1.51\n",
            "2025-01-15 14:40:49 Step 81/200: training loss=2.18\n",
            "2025-01-15 14:40:49 Step 80/200: training loss=1.65\n",
            "2025-01-15 14:40:49 Step 79/200: training loss=1.37\n",
            "2025-01-15 14:40:47 Step 78/200: training loss=1.51\n",
            "2025-01-15 14:40:44 Step 77/200: training loss=1.19\n",
            "2025-01-15 14:40:42 Step 76/200: training loss=1.44\n",
            "2025-01-15 14:40:42 Step 75/200: training loss=1.19\n",
            "2025-01-15 14:40:42 Step 74/200: training loss=1.35\n",
            "2025-01-15 14:40:37 Step 73/200: training loss=1.62\n",
            "2025-01-15 14:40:37 Step 72/200: training loss=1.10\n",
            "2025-01-15 14:40:34 Step 71/200: training loss=1.01\n",
            "2025-01-15 14:40:34 Step 70/200: training loss=1.17\n",
            "2025-01-15 14:40:34 Step 69/200: training loss=1.55\n",
            "2025-01-15 14:40:31 Step 68/200: training loss=1.14\n",
            "2025-01-15 14:40:31 Step 67/200: training loss=1.52\n",
            "2025-01-15 14:40:31 Step 66/200: training loss=1.41\n",
            "2025-01-15 14:40:29 Step 65/200: training loss=1.15\n",
            "2025-01-15 14:40:29 Step 64/200: training loss=1.40\n",
            "2025-01-15 14:40:26 Step 63/200: training loss=1.86\n",
            "2025-01-15 14:40:26 Step 62/200: training loss=1.48\n",
            "2025-01-15 14:40:26 Step 61/200: training loss=1.39\n",
            "2025-01-15 14:40:21 Step 60/200: training loss=1.35\n",
            "2025-01-15 14:40:21 Step 59/200: training loss=1.69\n",
            "2025-01-15 14:40:19 Step 58/200: training loss=1.30\n",
            "2025-01-15 14:40:16 Step 57/200: training loss=1.30\n",
            "2025-01-15 14:40:16 Step 56/200: training loss=1.71\n",
            "2025-01-15 14:40:14 Step 55/200: training loss=1.74\n",
            "2025-01-15 14:40:14 Step 54/200: training loss=0.91\n",
            "2025-01-15 14:40:14 Step 53/200: training loss=1.22\n",
            "2025-01-15 14:40:09 Step 52/200: training loss=1.25\n",
            "2025-01-15 14:40:08 Step 51/200: training loss=1.74\n",
            "2025-01-15 14:40:03 Step 50/200: training loss=1.04\n",
            "2025-01-15 14:40:03 Step 49/200: training loss=0.90\n",
            "2025-01-15 14:40:01 Step 48/200: training loss=1.24\n",
            "2025-01-15 14:39:58 Step 47/200: training loss=1.52\n",
            "2025-01-15 14:39:58 Step 46/200: training loss=1.31\n",
            "2025-01-15 14:39:56 Step 45/200: training loss=1.91\n",
            "2025-01-15 14:39:56 Step 44/200: training loss=1.31\n",
            "2025-01-15 14:39:56 Step 43/200: training loss=1.00\n",
            "2025-01-15 14:39:53 Step 42/200: training loss=1.27\n",
            "2025-01-15 14:39:53 Step 41/200: training loss=1.25\n",
            "2025-01-15 14:39:53 Step 40/200: training loss=1.78\n",
            "2025-01-15 14:39:50 Step 39/200: training loss=1.48\n",
            "2025-01-15 14:39:50 Step 38/200: training loss=1.30\n",
            "2025-01-15 14:39:48 Step 37/200: training loss=1.31\n",
            "2025-01-15 14:39:45 Step 36/200: training loss=1.51\n",
            "2025-01-15 14:39:43 Step 35/200: training loss=1.17\n",
            "2025-01-15 14:39:43 Step 34/200: training loss=1.37\n",
            "2025-01-15 14:39:43 Step 33/200: training loss=1.90\n",
            "2025-01-15 14:39:40 Step 32/200: training loss=1.74\n",
            "2025-01-15 14:39:40 Step 31/200: training loss=1.44\n",
            "2025-01-15 14:39:40 Step 30/200: training loss=1.21\n",
            "2025-01-15 14:39:38 Step 29/200: training loss=1.18\n",
            "2025-01-15 14:39:38 Step 28/200: training loss=1.34\n",
            "2025-01-15 14:39:38 Step 27/200: training loss=1.11\n",
            "2025-01-15 14:39:35 Step 26/200: training loss=1.21\n",
            "2025-01-15 14:39:35 Step 25/200: training loss=1.67\n",
            "2025-01-15 14:39:33 Step 24/200: training loss=1.33\n",
            "2025-01-15 14:39:33 Step 23/200: training loss=1.22\n",
            "2025-01-15 14:39:30 Step 22/200: training loss=1.35\n",
            "2025-01-15 14:39:27 Step 21/200: training loss=1.27\n",
            "2025-01-15 14:39:27 Step 20/200: training loss=1.55\n",
            "2025-01-15 14:39:25 Step 19/200: training loss=1.56\n",
            "2025-01-15 14:39:25 Step 18/200: training loss=1.50\n",
            "2025-01-15 14:39:25 Step 17/200: training loss=1.19\n",
            "2025-01-15 14:39:22 Step 16/200: training loss=0.93\n",
            "2025-01-15 14:39:22 Step 15/200: training loss=1.19\n",
            "2025-01-15 14:39:22 Step 14/200: training loss=1.94\n",
            "2025-01-15 14:39:20 Step 13/200: training loss=1.42\n",
            "2025-01-15 14:39:20 Step 12/200: training loss=1.44\n",
            "2025-01-15 14:39:15 Step 11/200: training loss=1.46\n",
            "2025-01-15 14:39:14 Step 10/200: training loss=1.30\n",
            "2025-01-15 14:39:14 Step 9/200: training loss=1.75\n",
            "2025-01-15 14:39:12 Step 8/200: training loss=1.77\n",
            "2025-01-15 14:39:12 Step 7/200: training loss=2.40\n",
            "2025-01-15 14:39:09 Step 6/200: training loss=1.85\n",
            "2025-01-15 14:39:09 Step 5/200: training loss=1.84\n",
            "2025-01-15 14:39:09 Step 4/200: training loss=1.30\n",
            "2025-01-15 14:39:07 Step 3/200: training loss=1.80\n",
            "2025-01-15 14:39:07 Step 2/200: training loss=2.03\n",
            "2025-01-15 14:39:06 Step 1/200: training loss=1.30\n",
            "2025-01-15 14:36:54 Fine-tuning job started\n",
            "2025-01-15 14:36:52 Files validated, moving job to queued state\n",
            "2025-01-15 14:35:12 Validating training file: file-VLM277YEE9fFFg64ZAnd4c\n",
            "2025-01-15 14:35:12 Created fine-tuning job: ftjob-wcNzlUfqWDW8juVhSHkCe2Dp\n",
            "######## Fine-tuned model ###########\n",
            "ft:gpt-4o-mini-2024-07-18:irlearning::Apyu0Syv\n",
            "#####################################\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "while True:\n",
        "  time.sleep(20)\n",
        "  try:\n",
        "      job_status = client.fine_tuning.jobs.retrieve(param_file_tune_job_id)\n",
        "      print(f\"------------ Job Status: {job_status.status} --------------\")\n",
        "\n",
        "      if job_status.status in [\"failed\", \"succeeded\", \"cancelled\"]:\n",
        "          print(\"Job Completed. Detailed Events list:\")\n",
        "          events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=param_file_tune_job_id)\n",
        "          for event in events:\n",
        "              print(f'{datetime.fromtimestamp(event.created_at)} {event.message}')\n",
        "\n",
        "          print(\"######## Fine-tuned model ###########\")\n",
        "          print(f\"{job_status.fine_tuned_model}\")\n",
        "          print(\"#####################################\")\n",
        "          break\n",
        "  except Exception as e:\n",
        "      print(f\"Error monitoring job: {e}\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QQLnxMH62r-"
      },
      "outputs": [],
      "source": [
        "# # Retrieve the state of a fine-tune\n",
        "\n",
        "# while client.fine_tuning.jobs.retrieve(param_file_tune_job_id).status != 'succeeded':\n",
        "#   sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RYlpT2WN62pV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f367e88f-e0b8-4be4-b9fd-bbce0856faeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'succeeded'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Retrieve the state of a fine-tune\n",
        "client.fine_tuning.jobs.retrieve(param_file_tune_job_id).status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WWuCemUD8dEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e787d1f-d1a4-4d2a-e460-7320c2aff9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ft:gpt-4o-mini-2024-07-18:irlearning::Apyu0Syv'\n"
          ]
        }
      ],
      "source": [
        "param_file_tune_model = client.fine_tuning.jobs.retrieve(param_file_tune_job_id).fine_tuned_model\n",
        "pprint(param_file_tune_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4n4o0CA9cMv"
      },
      "source": [
        "### We can delete the fine tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgM96Teu9Zi_"
      },
      "outputs": [],
      "source": [
        "# Delete a fine-tuned model (must be an owner of the org the model was created in)\n",
        "# result_delete = client.models.delete(param_file_tune_model)\n",
        "# pprint(result_delete)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d3dba0e64d14f919671e02a85b5b675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2257c2b668046acb477d325219b454f",
              "IPY_MODEL_a1f6885d9f314ac99ca8b3de514c6956",
              "IPY_MODEL_f67e88b47e214d90984d16dc59fe509e"
            ],
            "layout": "IPY_MODEL_dacd042b780f42bbb0c14642425f75d7"
          }
        },
        "d2257c2b668046acb477d325219b454f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22265c3bf8304c8dbcb12ec58f528991",
            "placeholder": "​",
            "style": "IPY_MODEL_8cfce3e5c08145839912b8a2ded2c6f4",
            "value": "vectorstore.zip: 100%"
          }
        },
        "a1f6885d9f314ac99ca8b3de514c6956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2b2ca415fe142f4878a2d70949dfb1e",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc919e6ae5f74640b03639dbd7b2dd0c",
            "value": 97198458
          }
        },
        "f67e88b47e214d90984d16dc59fe509e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9d9133470642eeb9b3b85b426c179a",
            "placeholder": "​",
            "style": "IPY_MODEL_671ba99566c743c3a30fd802ce6c5e78",
            "value": " 97.2M/97.2M [00:02&lt;00:00, 41.6MB/s]"
          }
        },
        "dacd042b780f42bbb0c14642425f75d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22265c3bf8304c8dbcb12ec58f528991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cfce3e5c08145839912b8a2ded2c6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2b2ca415fe142f4878a2d70949dfb1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc919e6ae5f74640b03639dbd7b2dd0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad9d9133470642eeb9b3b85b426c179a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671ba99566c743c3a30fd802ce6c5e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd0664da827c4c079ae239b1b0469f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e59e9762c8b0475997ee6716259d3a0e",
              "IPY_MODEL_6cf24a4c94114cbe912efced411d5a07",
              "IPY_MODEL_22a8bf67689a49d683b1160950486f65"
            ],
            "layout": "IPY_MODEL_53561ea3f6c94215ae4fec105fa4a110"
          }
        },
        "e59e9762c8b0475997ee6716259d3a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d19a888a38514636a6b1825ebb1c9b25",
            "placeholder": "​",
            "style": "IPY_MODEL_51dbb7fb34ab4d15b86db35a83b1fe99",
            "value": "question_answers_data_100.jsonl: 100%"
          }
        },
        "6cf24a4c94114cbe912efced411d5a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_206e3b22827d45239952fa50a862d294",
            "max": 276493,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f74111677cc64cde83fb79d581b6fff1",
            "value": 276493
          }
        },
        "22a8bf67689a49d683b1160950486f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d547d91c40eb466fab141eb115d4c35b",
            "placeholder": "​",
            "style": "IPY_MODEL_28282d95dcde462c894e9a42eeb670e7",
            "value": " 276k/276k [00:00&lt;00:00, 5.52MB/s]"
          }
        },
        "53561ea3f6c94215ae4fec105fa4a110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19a888a38514636a6b1825ebb1c9b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51dbb7fb34ab4d15b86db35a83b1fe99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "206e3b22827d45239952fa50a862d294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74111677cc64cde83fb79d581b6fff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d547d91c40eb466fab141eb115d4c35b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28282d95dcde462c894e9a42eeb670e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ad59ce4800048259eb604b7a9de5918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd4bd67bc74549b9a1aefe3ba3dd475d",
              "IPY_MODEL_f41b1259a3124c4fa2d499b5ad9a646e",
              "IPY_MODEL_06346c12fc14494493af114ccc4f697c"
            ],
            "layout": "IPY_MODEL_0fcaf09fd547450aa7eedb1341e77146"
          }
        },
        "dd4bd67bc74549b9a1aefe3ba3dd475d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68c81f61c2554d99bbb16d71d17de976",
            "placeholder": "​",
            "style": "IPY_MODEL_983ed2019aa04d2d9fb674d3f5172e7d",
            "value": "question_answers_data_30.jsonl: 100%"
          }
        },
        "f41b1259a3124c4fa2d499b5ad9a646e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_328fac496a3d41fabdf2c34519e27c83",
            "max": 81643,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13f692d724cd4fa0bc42689a0aa8c02e",
            "value": 81643
          }
        },
        "06346c12fc14494493af114ccc4f697c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_becf7ec56ace43e984a5fd6f6c6ef0c2",
            "placeholder": "​",
            "style": "IPY_MODEL_787b7405c7864af6bc67ec664d54c4a4",
            "value": " 81.6k/81.6k [00:00&lt;00:00, 1.94MB/s]"
          }
        },
        "0fcaf09fd547450aa7eedb1341e77146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68c81f61c2554d99bbb16d71d17de976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "983ed2019aa04d2d9fb674d3f5172e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "328fac496a3d41fabdf2c34519e27c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f692d724cd4fa0bc42689a0aa8c02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "becf7ec56ace43e984a5fd6f6c6ef0c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "787b7405c7864af6bc67ec664d54c4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}