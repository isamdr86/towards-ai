{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isamdr86/towards-ai/blob/main/notebooks/Audio_and_Realtime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wLLWaH8vFKn"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Audio_and_Realtime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmYF5uSLvFKr"
      },
      "source": [
        "# Adding Speech with OpenAI’s GPT4o Audio\n",
        "\n",
        "In this lesson, we see how to leverage the new audio capabilities of GPT4o with the \"gpt-4o-audio-preview\" model. We'll see how to write code that registers our voices, sends it to the model, and plays back the audio response. We'll also learn how to parse audio streaming output and play it as soon as the first audio chunks arrive. Last, we integrate this with the AI tutor knowledge base, getting to a script that listens to the user query, instructs the LLM to use the knowledge base to retrieve information for answering the query, and plays back the final audio response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noc-yviVvFKs"
      },
      "source": [
        "## Libraries and Environment Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGqT_gQjvFKs"
      },
      "source": [
        "The code has been tested with the following libraries installed:\n",
        "\n",
        "```\n",
        "chromadb==0.5.3\n",
        "huggingface-hub==0.26.2\n",
        "llama-index==0.10.49\n",
        "llama-index-embeddings-openai==0.1.11\n",
        "numpy==1.26.4\n",
        "openai==1.54.3\n",
        "PyAudio==0.2.14\n",
        "sounddevice==0.5.1\n",
        "wavio==0.0.9\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K5lj2rNHvFKs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyHAyDRnvFKu"
      },
      "source": [
        "## Load Knowledge Base and Create Retriever\n",
        "\n",
        "In this section, we download our 500 blog dataset and create a vector retriever with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "R-daZqvnvFKu"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4cHR3lGxvFKv",
        "outputId": "19213dbb-72b2-4393-dcf1-64f034b06274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vectorstore.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Download 500 blog dataset as knowledge base\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sAJgn-SpvFKv"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Load the vector store from the local storage\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create the index based on the vector store\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
        "\n",
        "# Create retriever\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DY-KCjPavFKv"
      },
      "outputs": [],
      "source": [
        "# Test the retriever with a query\n",
        "nodes = vector_retriever.retrieve(\"How does RAG work?\")\n",
        "for node in nodes:\n",
        "    print(node.metadata[\"title\"])\n",
        "    print(node.metadata[\"url\"])\n",
        "    print(\"-\" * 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B32txxbmvFKw"
      },
      "source": [
        "## Registering Audio and Generating Audio Responses with GPT4o\n",
        "\n",
        "In this section, we see how to (1) register audio from your microphone, (2) send the audio to GPT4o to generate an audio response, and (3) play the audio response and show its transcript."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libportaudio2\n",
        "!pip install sounddevice"
      ],
      "metadata": {
        "id": "Nz_BVAPQvUdc",
        "outputId": "77d2ede0-dcce-4577-d295-4bd9c3f893ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 65.3 kB of archives.\n",
            "After this operation, 223 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Fetched 65.3 kB in 0s (511 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 124565 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: sounddevice in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YScKuuy2vFKw"
      },
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import wavio\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "import tempfile\n",
        "import json\n",
        "import simpleaudio as sa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HezJNBL0vFKw"
      },
      "outputs": [],
      "source": [
        "def record_audio(key=\"q\", sample_rate=44100, channels=1):\n",
        "    \"\"\"Record audio from the microphone until the user sends the \"q\" key.\"\"\"\n",
        "    print(f\"Recording... Press '{key}' to stop.\")\n",
        "    audio_data = []\n",
        "\n",
        "    # Define a callback function to capture audio data\n",
        "    def callback(indata, frames, time, status):\n",
        "        audio_data.append(indata.copy())\n",
        "\n",
        "    # Open audio input stream and start recording\n",
        "    with sd.InputStream(samplerate=sample_rate, channels=channels, callback=callback):\n",
        "        while True:\n",
        "            if input() == key:\n",
        "                break\n",
        "    print(\"Stopped recording.\")\n",
        "\n",
        "    # Combine audio data and return as a numpy array\n",
        "    audio_data = np.concatenate(audio_data, axis=0)\n",
        "\n",
        "    # Save the audio to a temporary file\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as audio_file:\n",
        "        wavio.write(audio_file.name, audio_data, sample_rate, sampwidth=2)\n",
        "        audio_file_path = audio_file.name\n",
        "\n",
        "    return audio_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tvvXKyervFKx"
      },
      "outputs": [],
      "source": [
        "def send_audio_to_llm(audio_file_path, prompt):\n",
        "    \"\"\"Sends an audio file to the OpenAI API and returns the audio completion.\"\"\"\n",
        "    # Read the temp file and encode as base64\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"input_audio\",\n",
        "                    \"input_audio\": {\n",
        "                        \"data\": encoded_audio,\n",
        "                        \"format\": \"wav\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Send to OpenAI API\n",
        "    completion = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-audio-preview\",\n",
        "        modalities=[\"text\", \"audio\"],\n",
        "        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Ti96Ox1DvFKx"
      },
      "outputs": [],
      "source": [
        "def play_sound(pcm_bytes, sample_rate=24000, channels=1, sample_width=2):\n",
        "    \"\"\"Plays a sound from PCM bytes using simpleaudio\"\"\"\n",
        "    play_obj = sa.play_buffer(\n",
        "        pcm_bytes,\n",
        "        num_channels=channels,\n",
        "        bytes_per_sample=sample_width,\n",
        "        sample_rate=sample_rate\n",
        "    )\n",
        "    play_obj.wait_done()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sounddevice as sd\n",
        "\n",
        "# Get a list of available audio input devices\n",
        "devices = sd.query_devices()\n",
        "\n",
        "# Print the available devices for the user to choose from\n",
        "print(\"Available audio input devices:\")\n",
        "for i, device in enumerate(devices):\n",
        "    if device['max_input_channels'] > 0:  # Only show input devices\n",
        "        print(f\"{i}: {device['name']}\")"
      ],
      "metadata": {
        "id": "jpxwAuDWwf3J",
        "outputId": "47e95476-82ce-41fb-b24a-c98ddc03ecd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available audio input devices:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sounddevice as sd\n",
        "\n",
        "# Get a list of available audio input devices\n",
        "devices = sd.query_devices()\n",
        "\n",
        "# Print the available devices for the user to choose from\n",
        "print(\"Available audio input devices:\")\n",
        "for i, device in enumerate(devices):\n",
        "    if device['max_input_channels'] > 0:  # Only show input devices\n",
        "        print(f\"{i}: {device['name']}\")\n",
        "\n",
        "# Ask the user to select an input device\n",
        "selected_device_index = int(input(\"Select input device index: \"))\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "def record_audio(key=\"q\", sample_rate=44100, channels=1):\n",
        "    \"\"\"Record audio from the microphone until the user sends the \"q\" key.\"\"\"\n",
        "    print(f\"Recording... Press '{key}' to stop.\")\n",
        "    audio_data = []\n",
        "\n",
        "    # Define a callback function to capture audio data\n",
        "    def callback(indata, frames, time, status):\n",
        "        audio_data.append(indata.copy())\n",
        "\n",
        "    # Open audio input stream and start recording, using the selected device index\n",
        "    with sd.InputStream(samplerate=sample_rate,\n",
        "                       channels=channels,\n",
        "                       callback=callback,\n",
        "                       device=selected_device_index):  # Specify the device index\n",
        "        while True:\n",
        "            if input() == key:\n",
        "                break\n",
        "    print(\"Stopped recording.\")\n",
        "\n",
        "    # ... (rest of the function remains the same)\n",
        "\n",
        "    return audio_file_path"
      ],
      "metadata": {
        "id": "CCKD598VvwUu",
        "outputId": "d2235c25-03c9-46e6-dbe2-6053c15590a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available audio input devices:\n",
            "Select input device index: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Record audio until the user presses 'q'\n",
        "audio_file_path = record_audio()"
      ],
      "metadata": {
        "id": "6n_-S6FlwJou",
        "outputId": "a5453a09-5cad-4a47-c443-987e6bad717a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording... Press 'q' to stop.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PortAudioError",
          "evalue": "Error querying device 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-27969cc027ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Record audio until the user presses 'q'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maudio_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-f24f39087167>\u001b[0m in \u001b[0;36mrecord_audio\u001b[0;34m(key, sample_rate, channels)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Open audio input stream and start recording, using the selected device index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     with sd.InputStream(samplerate=sample_rate, \n\u001b[0m\u001b[1;32m     29\u001b[0m                        \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                        \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m         \"\"\"\n\u001b[0;32m-> 1440\u001b[0;31m         _StreamBase.__init__(self, kind='input', wrap_callback='array',\n\u001b[0m\u001b[1;32m   1441\u001b[0m                              **_remove_self(locals()))\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samplesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 _get_stream_parameters(kind, device, channels, dtype, latency,\n\u001b[0m\u001b[1;32m    829\u001b[0m                                        extra_settings, samplerate)\n\u001b[1;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m_get_stream_parameters\u001b[0;34m(kind, device, channels, dtype, latency, extra_settings, samplerate)\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0msamplerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36mquery_devices\u001b[0;34m(device, kind)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPa_GetDeviceInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mPortAudioError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Error querying device {device}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructVersion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0mname_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPortAudioError\u001b[0m: Error querying device 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IFoRdLqZvFKy",
        "outputId": "7546d4ce-de0b-4680-8df9-5300c95b5f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording... Press 'q' to stop.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PortAudioError",
          "evalue": "Error querying device 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-22354e8a5dd1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Record audio until the user presses 'q'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maudio_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize OpenAI API client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopenai_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-f24f39087167>\u001b[0m in \u001b[0;36mrecord_audio\u001b[0;34m(key, sample_rate, channels)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Open audio input stream and start recording, using the selected device index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     with sd.InputStream(samplerate=sample_rate, \n\u001b[0m\u001b[1;32m     29\u001b[0m                        \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                        \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m         \"\"\"\n\u001b[0;32m-> 1440\u001b[0;31m         _StreamBase.__init__(self, kind='input', wrap_callback='array',\n\u001b[0m\u001b[1;32m   1441\u001b[0m                              **_remove_self(locals()))\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_samplesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplerate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 _get_stream_parameters(kind, device, channels, dtype, latency,\n\u001b[0m\u001b[1;32m    829\u001b[0m                                        extra_settings, samplerate)\n\u001b[1;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36m_get_stream_parameters\u001b[0;34m(kind, device, channels, dtype, latency, extra_settings, samplerate)\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0msamplerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sounddevice.py\u001b[0m in \u001b[0;36mquery_devices\u001b[0;34m(device, kind)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPa_GetDeviceInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mPortAudioError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Error querying device {device}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructVersion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0mname_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPortAudioError\u001b[0m: Error querying device 0"
          ]
        }
      ],
      "source": [
        "# Record audio until the user presses 'q'\n",
        "audio_file_path = record_audio()\n",
        "\n",
        "# Initialize OpenAI API client\n",
        "openai_client = OpenAI()\n",
        "\n",
        "# Print transcription result\n",
        "prompt = \"Transcribe the attached recording. Write only the transcription and nothing else.\"\n",
        "completion = send_audio_to_llm(audio_file_path, prompt)\n",
        "print(completion.choices[0].message.audio.transcript)\n",
        "\n",
        "# Play the audio response\n",
        "pcm_bytes = base64.b64decode(completion.choices[0].message.audio.data)\n",
        "play_sound(pcm_bytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlPRr7V7vFKy"
      },
      "source": [
        "## Using Streaming Outputs\n",
        "\n",
        "In this section we see how to leveraging streaming outputs of the OpenAI API to retrieve the audio response chunk by chunk. This allows us to play the response audio with lower latency as we play the first bytes as soon as we receive them intead of waiting for the whole audio output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyaudio"
      ],
      "metadata": {
        "id": "x85GMUxnv3DB",
        "outputId": "2a41e6c7-b2e6-4c7c-8fd0-32b8cd38e7f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyaudio\n",
            "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyaudio\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pyaudio)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "041fmQUrvFKy",
        "outputId": "4f7c824f-e6a0-4d2b-885e-28876db982a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyaudio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-07d958faba10>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pyaudio\n",
        "import threading\n",
        "import queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t6FhRO3vFKy"
      },
      "outputs": [],
      "source": [
        "def play_sound_from_queue(pcm_queue, sample_rate=24000, channels=1, sample_width=2):\n",
        "    \"\"\"\n",
        "    Play PCM audio data from a queue that is being filled over time.\n",
        "\n",
        "    Args:\n",
        "        pcm_queue: A Queue object from which PCM data is read.\n",
        "    \"\"\"\n",
        "    p = pyaudio.PyAudio()\n",
        "    format = p.get_format_from_width(sample_width)\n",
        "\n",
        "    # Open a blocking stream\n",
        "    stream = p.open(format=format,\n",
        "                    channels=channels,\n",
        "                    rate=sample_rate,\n",
        "                    output=True)\n",
        "\n",
        "    # Read data from the queue and write to the stream\n",
        "    while True:\n",
        "        data = pcm_queue.get()\n",
        "        if data is None:\n",
        "            break  # No more data to play\n",
        "        stream.write(data)\n",
        "\n",
        "    # Clean up\n",
        "    stream.stop_stream()\n",
        "    stream.close()\n",
        "    p.terminate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aptWdiiQvFKz"
      },
      "outputs": [],
      "source": [
        "def play_sound_and_print_transcript(stream):\n",
        "    \"\"\"\n",
        "    Starting from a stream of audio chunks (the response to the LLM call),\n",
        "    plays the response audio and prints its transcript.\n",
        "    \"\"\"\n",
        "    pcm_queue = queue.Queue()\n",
        "    has_playback_started = False\n",
        "    for chunk in stream:\n",
        "        if hasattr(chunk.choices[0].delta, \"audio\"):\n",
        "            chunk_audio = chunk.choices[0].delta.audio\n",
        "            if \"transcript\" in chunk_audio:\n",
        "                print(chunk_audio[\"transcript\"], end=\"\") # Print the transcript\n",
        "            elif \"data\" in chunk_audio:\n",
        "                pcm_bytes = base64.b64decode(chunk_audio[\"data\"])\n",
        "                pcm_queue.put(pcm_bytes) # Add the audio data to the queue\n",
        "                if not has_playback_started:\n",
        "                    # Start the playback thread\n",
        "                    playback_thread = threading.Thread(target=play_sound_from_queue, args=(pcm_queue,))\n",
        "                    playback_thread.start()\n",
        "                    has_playback_started = True\n",
        "    pcm_queue.put(None) # Signal end of data\n",
        "    playback_thread.join() # Wait for playback to finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWJEk091vFK0"
      },
      "outputs": [],
      "source": [
        "# Get response from GPT4o (i.e. a stream of chunks of audio)\n",
        "with open(audio_file_path, \"rb\") as audio_file:\n",
        "    encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
        "\n",
        "# Prepare messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": prompt\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"input_audio\",\n",
        "                \"input_audio\": {\n",
        "                    \"data\": encoded_audio,\n",
        "                    \"format\": \"wav\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Get streaming response from the LLM\n",
        "stream = openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o-audio-preview\",\n",
        "    modalities=[\"text\", \"audio\"],\n",
        "    audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "    messages=messages,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "# Play the audio response and print the transcript\n",
        "play_sound_and_print_transcript(stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkvwxkhCvFK0"
      },
      "source": [
        "## Integrating Audio Inputs and Outputs with RAG\n",
        "\n",
        "In this section, we see how to (1) define the tool that retrieves relevant information from our knowledge base, (2) send the user query to the LLM specifying the available tools, (3) manage the LLM response if it asks to use a tool, (4) get the final audio response via streaming from the LLM leveraging the tool response, and (5) play the audio response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqGEZna6vFK0"
      },
      "outputs": [],
      "source": [
        "# This function will be used as tool for the LLM to retrieve resources\n",
        "def retrieve_resources(query: str) -> str:\n",
        "    \"\"\"Given a query, retrieve relevant resources and return them as a formatted string.\"\"\"\n",
        "    nodes = vector_retriever.retrieve(query)\n",
        "\n",
        "    context_text = \"\"\n",
        "    for i, node in enumerate(nodes):\n",
        "        context_text += f\"<resource-{i+1}>\" + \"\\n\"\n",
        "        context_text += \"<resource-title>\" + node.node.metadata[\"title\"] + \"</resource-title>\" + \"\\n\\n\"\n",
        "        context_text += \"<resource-text>\" + \"\\n\" + node.node.text + \"\\n\" + \"</resource-text>\" + \"\\n\"\n",
        "        context_text += f\"</resource-{i+1}>\" + \"\\n\\n\"\n",
        "    context_text = context_text.strip()\n",
        "\n",
        "    return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfW4GvuPvFK1"
      },
      "outputs": [],
      "source": [
        "# Define the tools for the LLM\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"retrieve_resources\",\n",
        "            \"description\": \"Given a query, find resources that are relevant to the query and useful for answering it. It leverages an internal knowledge base.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"A query that will be used (via embeddings similarity search) to find relevant resources.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "                \"additionalProperties\": False\n",
        "            },\n",
        "            \"response\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"A textual representation of the resources found that are relevant to the query.\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz17ABJZvFK1"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant whose job is answering user queries about artificial intelligence topics.\n",
        "Leverage the \"retrieve_resources\" tool to find resources based on the user's query.\n",
        "You can use the tool at most once per user query.\n",
        "Always leverage the retrieved resources to provide a helpful response.\n",
        "If you can't find useful information, don't use your knowledge to make up an answer, just say that you can't find the information in your knowledge base.\n",
        "Speak fast.\n",
        "Be very concise. Answer with at most 50 words.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h78QOXt-vFK1"
      },
      "outputs": [],
      "source": [
        "def send_audio_to_llm(audio_file_path, system_prompt):\n",
        "    \"\"\"Sends an audio file to the OpenAI API and returns the audio completion.\"\"\"\n",
        "    # Read the temp file and encode as base64\n",
        "    with open(audio_file_path, \"rb\") as audio_file:\n",
        "        encoded_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n",
        "\n",
        "    # Define the messages to send to the LLM\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"input_audio\",\n",
        "                    \"input_audio\": {\n",
        "                        \"data\": encoded_audio,\n",
        "                        \"format\": \"wav\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Send to OpenAI API\n",
        "    completion = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-audio-preview\",\n",
        "        modalities=[\"text\", \"audio\"],\n",
        "        audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "    )\n",
        "\n",
        "    return completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7mV_DNyvFK3"
      },
      "outputs": [],
      "source": [
        "completion = send_audio_to_llm(audio_file_path, system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_gWOG6WvFK3"
      },
      "outputs": [],
      "source": [
        "# Show the response (spoiler: it's a function call)\n",
        "completion.choices[0].to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAw9Z1MHvFK4"
      },
      "outputs": [],
      "source": [
        "def manage_tool_call(completion):\n",
        "    \"\"\"\n",
        "    If the LLM completion contains a tool call, retrieve the resources and continue the conversation.\n",
        "    The returned conversation is in the form of a stream.\n",
        "    \"\"\"\n",
        "    if completion.choices[0].finish_reason == \"tool_calls\":\n",
        "        tool_call_id = completion.choices[0].message.tool_calls[0].id\n",
        "        tool_name = completion.choices[0].message.tool_calls[0].function.name # not used\n",
        "        tool_query = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)[\"query\"]\n",
        "        resources = retrieve_resources(tool_query)\n",
        "\n",
        "        new_messages = messages + [\n",
        "            completion.choices[0].message,\n",
        "            {\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": json.dumps({\n",
        "                    \"query\": tool_query,\n",
        "                    \"resources\": resources,\n",
        "                }),\n",
        "                \"tool_call_id\": tool_call_id\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        stream = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-audio-preview\",\n",
        "            modalities=[\"text\", \"audio\"],\n",
        "            audio={\"voice\": \"alloy\", \"format\": \"pcm16\"},\n",
        "            messages=new_messages,\n",
        "            stream=True,\n",
        "        )\n",
        "\n",
        "        return stream\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOPWgVMpvFK4"
      },
      "outputs": [],
      "source": [
        "# Run the tool call and play the audio response\n",
        "stream = manage_tool_call(completion)\n",
        "play_sound_and_print_transcript(stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOKqwsrHvFK4"
      },
      "source": [
        "## Putting All Together\n",
        "\n",
        "Last, we put everything together in a single script so that (1) the user registers its question via audio, (2) the LLM generates a final audio response leveraging the retrieval tool, and (3) the audio response is played via streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbHDEmZivFK5"
      },
      "outputs": [],
      "source": [
        "# 1. Record audio until the user presses 'q'\n",
        "audio_file_path = record_audio()\n",
        "\n",
        "# 2. Send audio to GPT4o\n",
        "completion = send_audio_to_llm(audio_file_path, system_prompt)\n",
        "\n",
        "# 3. Manage tool call\n",
        "# NB: We're assuming that the first LLM response is always a tool call!\n",
        "stream = manage_tool_call(completion)\n",
        "\n",
        "# 4. Play final response\n",
        "play_sound_and_print_transcript(stream)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llamaindexkernel",
      "language": "python",
      "name": "llamaindexkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}