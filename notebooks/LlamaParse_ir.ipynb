{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a87748afbdc84119b96bcccad3f251bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed80d6a97a464a01a517916dffe7aa69",
              "IPY_MODEL_aa792a6391d944aebecaade7ebdfc119",
              "IPY_MODEL_6ef2e304aea14295bdc97e11cd0219b3"
            ],
            "layout": "IPY_MODEL_4377bfbf149d4d6299920ed108e2719f"
          }
        },
        "ed80d6a97a464a01a517916dffe7aa69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ce95401d3fb4e559ebe64dc17eb245a",
            "placeholder": "​",
            "style": "IPY_MODEL_c24c3c83060440c3a6df94050c132054",
            "value": "research_papers_llamaparse.zip: 100%"
          }
        },
        "aa792a6391d944aebecaade7ebdfc119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_457f954e7b354e2185157b47ed22c2d3",
            "max": 13584308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9b9099a23434c7a847b54fa5341bc6f",
            "value": 13584308
          }
        },
        "6ef2e304aea14295bdc97e11cd0219b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81c54ad236fc46f58e22062dcbbdd7bf",
            "placeholder": "​",
            "style": "IPY_MODEL_af33cdb9580e4fa9819ade5b190d4087",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 24.9MB/s]"
          }
        },
        "4377bfbf149d4d6299920ed108e2719f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ce95401d3fb4e559ebe64dc17eb245a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c24c3c83060440c3a6df94050c132054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "457f954e7b354e2185157b47ed22c2d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b9099a23434c7a847b54fa5341bc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81c54ad236fc46f58e22062dcbbdd7bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af33cdb9580e4fa9819ade5b190d4087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isamdr86/towards-ai/blob/main/notebooks/LlamaParse_ir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3iSJpxmEML0w",
        "outputId": "d8ae3eaa-0e51-4e99-e0b0-09550943b2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.10-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.10 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.10.post1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from llama-parse) (8.1.8)\n",
            "Requirement already satisfied: pydantic!=2.10 in /usr/local/lib/python3.10/dist-packages (from llama-parse) (2.10.4)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.59.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (3.11.11)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.10->llama-index) (1.17.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.8-py3-none-any.whl.metadata (860 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.10->llama-parse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.10->llama-parse) (2.27.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi<2025.0.0,>=2024.7.4 in /usr/local/lib/python3.10/dist-packages (from llama-cloud>=0.1.5->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2024.12.14)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.10->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.10->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.10->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama-index)\n",
            "  Downloading marshmallow-3.24.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.10->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.10->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Downloading llama_index-0.12.10-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.2-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.8-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.24.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, llama-cloud, dataclasses-json, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.8 llama-index-0.12.10 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.10.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.13 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.2 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.19 marshmallow-3.24.2 mypy-extensions-1.0.0 pypdf-5.1.0 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U llama-index llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = userdata.get('llama_api_key')"
      ],
      "metadata": {
        "id": "n9OKejSAM1Kj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Research paper dataset from HuggingFace Hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"research_papers_llamaparse.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "a87748afbdc84119b96bcccad3f251bf",
            "ed80d6a97a464a01a517916dffe7aa69",
            "aa792a6391d944aebecaade7ebdfc119",
            "6ef2e304aea14295bdc97e11cd0219b3",
            "4377bfbf149d4d6299920ed108e2719f",
            "8ce95401d3fb4e559ebe64dc17eb245a",
            "c24c3c83060440c3a6df94050c132054",
            "457f954e7b354e2185157b47ed22c2d3",
            "a9b9099a23434c7a847b54fa5341bc6f",
            "81c54ad236fc46f58e22062dcbbdd7bf",
            "af33cdb9580e4fa9819ade5b190d4087"
          ]
        },
        "id": "Xs5WjC01NCiy",
        "outputId": "92dd6195-c50c-48fb-bfc6-5ba4b62bd2e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "research_papers_llamaparse.zip:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a87748afbdc84119b96bcccad3f251bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip research_papers_llamaparse.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRNqM8cIPQC_",
        "outputId": "7dd4eeca-5284-4553-8039-0a19bfd0c201"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  research_papers_llamaparse.zip\n",
            "   creating: research_papers_llamaparse/\n",
            "  inflating: research_papers_llamaparse/2106.09685v2.pdf  \n",
            "  inflating: research_papers_llamaparse/2404.19756v2.pdf  \n",
            "  inflating: research_papers_llamaparse/2405.07437v2.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "WC9urbdDOABf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse directory to LlamaParse"
      ],
      "metadata": {
        "id": "itMgf7Dsptw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaParse Implemetation\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "#Parser\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "-JuFlFt2NcJF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_extractor = {\".pdf\": parser}\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/research_papers_llamaparse\", file_extractor=file_extractor).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrPv4GtFODxP",
        "outputId": "6e4222be-e7fc-48e4-a508-46ae25457828"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 48c4c964-ff69-455d-863c-168966585a2c\n",
            "......Started parsing the file under job_id 9d8a92a8-683e-406c-a3c8-e8559344b58d\n",
            ".....................Started parsing the file under job_id 768fbe1c-cf2a-4952-978f-e52dddc0ec40\n",
            ".."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvWyrtWhPgkZ",
        "outputId": "f1fc693e-c777-4e7e-a1d2-53b19d8931df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='7101c2cd-7424-493a-969e-fa5094aaca03', embedding=None, metadata={'file_path': '/content/research_papers_llamaparse/2106.09685v2.pdf', 'file_name': '2106.09685v2.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2025-01-09', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\\n\\nEdward Hu∗ Yelong Shen∗ Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen\\n\\nMicrosoft Corporation\\n\\n{edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com\\n\\nyuanzhil@andrew.cmu.edu\\n\\n(Version 2)\\n\\narXiv:2106.09685v2 [cs.CL] 16 Oct 2021\\n\\n# ABSTRACT\\n\\nAn important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\\n\\n# 1 INTRODUCTION\\n\\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere “inconvenience” for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters.\\n\\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques\\n\\n∗Equal contribution.\\n\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.\\n\\n# Figure 1: Our reparametrization.\\n\\nWe only train A and B', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaParse JSON Output"
      ],
      "metadata": {
        "id": "fCfzTVX0pqhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LlamaParse in JSON Mode for PDF Reading\n",
        "\n",
        "import glob\n",
        "pdf_files = glob.glob(\"/content/research_papers_llamaparse/*.pdf\")\n",
        "\n",
        "parser = LlamaParse(verbose=True)\n",
        "\n",
        "json_objs=[]\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "  json_objs.extend(parser.get_json_result(pdf_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBb4VjgPPiWq",
        "outputId": "fb2f56e6-6bf4-43fb-8f33-54bde59cf045"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 31155ac7-5814-42fd-ac91-248c6aff7f40\n",
            "Started parsing the file under job_id 0d494fad-791d-46ea-8328-10f4f198d963\n",
            "Started parsing the file under job_id 137c3221-61da-4b64-b7a5-eaa9452527c1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "A9HexIWZWCpV",
        "outputId": "3d79b7d1-39d9-4655-9955-68aaa0d73d38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                              LORA:              LOW-RANK                     ADAPTATION                       OF       LARGE              LAN-\\n                              GUAGE MODELS\\n                               Edward Hu∗             Yelong Shen∗            Phillip Wallis          Zeyuan Allen-Zhu\\n                               Yuanzhi Li            Shean Wang             Lu Wang            Weizhu Chen\\n                               Microsoft Corporation\\n                               {edwardhu, yeshe, phwallis, zeyuana,\\n                               yuanzhil, swang, luw, wzchen}@microsoft.com\\n                               yuanzhil@andrew.cmu.edu\\n                               (Version 2)\\narXiv:2106.09685v2  [cs.CL]  16 Oct 2021\\n                                                                                  ABSTRACT\\n                                        An important paradigm of natural language processing consists of large-scale pre-\\n                                        training on general domain data and adaptation to particular tasks or domains. As\\n                                        we pre-train larger models, full fine-tuning, which retrains all model parameters,\\n                                        becomes less feasible. Using GPT-3 175B as an example – deploying indepen-\\n                                        dent instances of fine-tuned models, each with 175B parameters, is prohibitively\\n                                        expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-\\n                                        trained model weights and injects trainable rank decomposition matrices into each\\n                                        layer of the Transformer architecture, greatly reducing the number of trainable pa-\\n                                        rameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam,\\n                                        LoRA can reduce the number of trainable parameters by 10,000 times and the\\n                                        GPU memory requirement by 3 times. LoRA performs on-par or better than fine-\\n                                        tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\n                                        ing fewer trainable parameters, a higher training throughput, and, unlike adapters,\\n                                        no additional inference latency. We also provide an empirical investigation into\\n                                        rank-deficiency in language model adaptation, which sheds light on the efficacy of\\n                                        LoRA. We release a package that facilitates the integration of LoRA with PyTorch\\n                                        models and provide our implementations and model checkpoints for RoBERTa,\\n                                        DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\\n\\n                              1\\n                                   INTRODUCTION\\n                             Many applications in natural language processing rely on adapt-                                                                 f(x)\\n                             ing one large-scale, pre-trained language model to multiple down-                              h\\n                             stream applications. Such adaptation is usually done via fine-tuning,\\n                             which updates all the parameters of the pre-trained model. The ma-                                                          Pretrained\\n                             jor downside of fine-tuning is that the new model contains as many                        Pretrained          𝐵 = 0          Weights\\n                             parameters as in the original model. As larger models are trained                          Weights               𝑟\\n                             every few months, this changes from a mere “inconvenience” for                                                              𝑊  ℝ𝑑×𝑑∈\\n                             GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a                        𝑊 ∈ ℝ𝑑×𝑑      𝐴 = 𝒩(0, 𝜎2)\\n                             critical deployment challenge for GPT-3 (Brown et al., 2020) with                                                                        𝑑\\n                             175 billion trainable parameters.1                                                                     𝑑                        x\\n                                                                                                                            x\\n                             Many sought to mitigate this by adapting only some parameters or\\n                             learning external modules for new tasks. This way, we only need\\n                             to store and load a small number of task-specific parameters in ad-                    Figure 1: Our reparametriza-.\\n                                                                                                                    tion. We only train A and B\\n                             dition to the pre-trained model for each task, greatly boosting the\\n                             operational efficiency when deployed. However, existing techniques\\n                                 ∗Equal contribution.\\n                                 0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n                                 1While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its perfor-\\n                              mance significantly as shown in Appendix A.\\n\\n                                                                                         1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][4]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "fN075z18fAHI",
        "outputId": "9213e960-9d84-4175-ea91-9a88dcfc5a02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'guarantees that we do not introduce any additional latency during inference compared to a fine-tuned\\nmodel by construction.\\n\\n4.2     APPLYING LORA TO TRANSFORMER\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\nthe self-attention module (Wq , Wk, Wv × dmodel, even though the output dimension is usually sliced\\n                                                                                                   as a single matrix of dimension dmodel , Wo) and two in the MLP module. We treat Wq (or Wk, Wv )\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\nand parameter-efficiency.We further study the effect on adapting different types of attention weight\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\nlayers, LayerNorm layers, and biases to a future work.\\n\\nPractical Benefits and Limitations.                 The most significant benefit comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to 2/3 if r              dmodel as we do not need to store the optimizer states for the frozen\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000× (from 350GB to 35MB)4. This allows us to train with signifi-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the fly on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\nduring training on GPT-3 175B compared to full fine-tuning5 as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n\\n5\\n      EMPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). Specifically, we evaluate on the GLUE (Wang et al., 2019) benchmark\\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n\\n5.1     BASELINES\\n\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their\\nreported numbers whenever possible. This, however, means that some baselines might only appear\\nin certain experiments.\\nFine-Tuning (FT) is a common approach for adaptation. During fine-tuning, the model is initialized\\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\\nvariant is to update only some layers while freezing others. We include one such baseline reported\\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers (FTTop2).\\n    4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 ≈ 354GB as opposed to 100 * 350GB ≈ 35TB.\\n    5For GPT-3 175B, the training throughput for full fine-tuning is 32.5 tokens/s per V100 GPU; with the same\\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\\n\\n                                                               5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KAN 7th page complete extracted information\n",
        "json_objs[0]['pages'][6]['text']"
      ],
      "metadata": {
        "id": "TXGAykARXWdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "ae6d684c-e82a-4907-c9a0-276336ce7078"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'   Model & Method                 # Trainable                            E2E NLG Challenge\\n                                   Parameters        BLEU          NIST         MET         ROUGE-L     CIDEr\\n   GPT-2 M (FT)*                     354.92M          68.2          8.62         46.2           71.0     2.47\\n   GPT-2 M (AdapterL)*                  0.37M         66.3          8.41         45.0           69.8     2.40\\n   GPT-2 M (AdapterL)*                 11.09M         68.9          8.71         46.1           71.3     2.47\\n   GPT-2 M (AdapterH)                  11.09M       67.3±.6      8.50±.07      46.0±.2        70.7±.2  2.44±.01\\n   GPT-2 M (FTTop2)*\\n                                       25.19M         68.1          8.59         46.0           70.8     2.41\\n   GPT-2 M (PreLayer)*                  0.35M         69.7          8.81         46.1           71.4     2.49\\n   GPT-2 M (LoRA)                       0.35M       70.4±.1      8.85±.02      46.8±.2        71.8±.1  2.53±.02\\n   GPT-2 L (FT)*\\n                                     774.03M          68.5          8.78         46.0           69.9     2.45\\n   GPT-2 L (AdapterL)                   0.88M       69.1±.1      8.68±.03      46.3±.0        71.4±.2   2.49±.0\\n   GPT-2 L (AdapterL)\\n                                       23.00M       68.9±.3      8.70±.04      46.1±.1        71.3±.2  2.45±.02\\n   GPT-2 L (PreLayer)*\\n                                        0.77M         70.3          8.85         46.2           71.7     2.47\\n   GPT-2 L (LoRA)                       0.77M       70.4±.1      8.89±.02      46.8±.2        72.0±.2  2.47±.02\\n\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\\nor fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n\\n5.2    ROBERTA BASE/LARGE\\n\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latter’s task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efficient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs\\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is\\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n\\n5.3    DEBERTA XXL\\n\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\\nfine-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\\nSee Section D.2 for details on the hyperparameters used.\\n\\n5.4    GPT-2 MEDIUM/LARGE\\n\\nHaving shown that LoRA can be a competitive alternative to full fine-tuning on NLU, we hope to\\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\\ninclude a list of the hyperparameters used in Section D.3.\\n\\n                                                             7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Table information\n",
        "json_objs[0]['pages'][6]['items'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEnTFCn2pgB7",
        "outputId": "a91c3931-f9f1-45e2-dc5d-d628cc5b2400"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'text',\n",
              " 'value': 'RoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin et al., 2019a) and boosted the latter’s task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020) and evaluate the performance of different efficient adaptation approaches on tasks from the GLUE benchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.',\n",
              " 'md': 'RoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin et al., 2019a) and boosted the latter’s task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020) and evaluate the performance of different efficient adaptation approaches on tasks from the GLUE benchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.',\n",
              " 'bBox': {'x': 108, 'y': 96, 'w': 396, 'h': 422.96}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][3]['items'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2KgsfBKrxME",
        "outputId": "bd6f2751-4ba2-4749-b81d-681cc869eb29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'heading',\n",
              " 'lvl': 1,\n",
              " 'value': '4 OUR METHOD',\n",
              " 'md': '# 4 OUR METHOD',\n",
              " 'bBox': {'x': 108, 'y': 252, 'w': 199.98, 'h': 508.96}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKy6jmjLyG3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTfBbY2l2czn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}